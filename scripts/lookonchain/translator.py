"""
æ–‡ç« ç¿»è¯‘å’Œæ€»ç»“æ¨¡å—
ä½¿ç”¨ GLM API å°†è‹±æ–‡æ–‡ç« ç¿»è¯‘ä¸ºä¸­æ–‡å¹¶ç”Ÿæˆæ‘˜è¦
"""

import json
import time
import sys
import os
from typing import Dict, Tuple, Optional, Any
from openai import OpenAI

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥ glm_logger
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
try:
    from glm_logger import GLMLogger, GLMClientWrapper
except ImportError:
    # å¦‚æœ glm_logger ä¸å­˜åœ¨ï¼Œåˆ›å»ºç®€åŒ–ç‰ˆæœ¬
    print("Warning: glm_logger not found, using simplified version")
    
    class GLMLogger:
        def __init__(self):
            self.stats = {"total_calls": 0, "successful_calls": 0, "failed_calls": 0, 
                         "total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0}
        
        def get_daily_stats(self):
            return self.stats
    
    class GLMClientWrapper:
        def __init__(self, api_key, base_url, logger):
            self.client = OpenAI(api_key=api_key, base_url=base_url)
            self.logger = logger
        
        def chat_completions_create(self, **kwargs):
            try:
                response = self.client.chat.completions.create(**kwargs)
                self.logger.stats["total_calls"] += 1
                self.logger.stats["successful_calls"] += 1
                if hasattr(response, 'usage') and response.usage:
                    self.logger.stats["total_tokens"] += response.usage.total_tokens
                    self.logger.stats["prompt_tokens"] += response.usage.prompt_tokens
                    self.logger.stats["completion_tokens"] += response.usage.completion_tokens
                return response
            except Exception as e:
                self.logger.stats["total_calls"] += 1
                self.logger.stats["failed_calls"] += 1
                raise e
from .config import (
    GLM_API_KEY, GLM_API_BASE, GLM_MODEL,
    TRANSLATION_TEMPERATURE, SUMMARY_TEMPERATURE,
    MAX_TOKENS_TRANSLATION, MAX_TOKENS_SUMMARY
)


def extract_content_from_response(response: Any, debug_context: str = "") -> Optional[str]:
    """
    ç¨³å¥åœ°ä»GLM APIå“åº”ä¸­æå–æ–‡æœ¬å†…å®¹
    æ”¯æŒå¤šç§å¯èƒ½çš„å“åº”æ ¼å¼ï¼Œå¹¶æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
    """
    if not response:
        print(f"âš ï¸ [{debug_context}] å“åº”ä¸ºç©º")
        return None
    
    try:
        # å°è¯•æ ‡å‡†OpenAIæ ¼å¼ï¼šresponse.choices[0].message.content
        if hasattr(response, 'choices') and response.choices:
            choice = response.choices[0]
            if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                content = choice.message.content
                if content and content.strip():
                    return content.strip()
                else:
                    print(f"âš ï¸ [{debug_context}] choices[0].message.content ä¸ºç©º")
        
        # å°è¯•ç›´æ¥çš„contentå­—æ®µ
        if hasattr(response, 'content'):
            content = response.content
            if content and content.strip():
                return content.strip()
            else:
                print(f"âš ï¸ [{debug_context}] response.content ä¸ºç©º")
        
        # å°è¯•textå­—æ®µ
        if hasattr(response, 'text'):
            text = response.text
            if text and text.strip():
                return text.strip()
            else:
                print(f"âš ï¸ [{debug_context}] response.text ä¸ºç©º")
        
        # å°è¯•å­—å…¸æ ¼å¼
        if isinstance(response, dict):
            for key in ['content', 'text', 'output', 'result']:
                if key in response and response[key]:
                    content = str(response[key]).strip()
                    if content:
                        return content
                    else:
                        print(f"âš ï¸ [{debug_context}] response['{key}'] ä¸ºç©º")
        
        # å¦‚æœæ‰€æœ‰æ ‡å‡†æ ¼å¼éƒ½å¤±è´¥ï¼Œè®°å½•å®Œæ•´å“åº”ç»“æ„ç”¨äºè°ƒè¯•
        print(f"ğŸ” [{debug_context}] æ— æ³•è§£æå“åº”ï¼Œå“åº”ç»“æ„:")
        if hasattr(response, '__dict__'):
            print(f"    å±æ€§: {list(response.__dict__.keys())}")
        elif isinstance(response, dict):
            print(f"    å­—å…¸é”®: {list(response.keys())}")
        else:
            print(f"    ç±»å‹: {type(response)}")
            print(f"    å†…å®¹: {str(response)[:200]}...")
        
        return None
        
    except Exception as e:
        print(f"âŒ [{debug_context}] è§£æå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


class ChineseTranslator:
    """ä¸­æ–‡ç¿»è¯‘å’Œæ€»ç»“ç”Ÿæˆå™¨"""
    
    def __init__(self, glm_api_key: str = None):
        self.api_key = glm_api_key or GLM_API_KEY
        self.client = None
        self.logger = None
        
        if self.api_key:
            try:
                # åˆå§‹åŒ–GLMæ—¥å¿—è®°å½•å™¨
                self.logger = GLMLogger()
                
                # ä½¿ç”¨åŒ…è£…å®¢æˆ·ç«¯ï¼Œè‡ªåŠ¨è®°å½•APIè°ƒç”¨
                self.client = GLMClientWrapper(
                    api_key=self.api_key,
                    base_url=GLM_API_BASE,
                    logger=self.logger
                )
                print("âœ… GLMç¿»è¯‘å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ GLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None
                self.logger = None
        else:
            print("âŒ ç¼ºå°‘GLM APIå¯†é’¥ï¼Œç¿»è¯‘åŠŸèƒ½å°†ä¸å¯ç”¨")
    
    def translate_to_chinese(self, english_content: str, title: str = "") -> Optional[str]:
        """å°†è‹±æ–‡å†…å®¹ç¿»è¯‘ä¸ºä¸­æ–‡"""
        if not self.client:
            print("âŒ ç¿»è¯‘å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        
        try:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸å’ŒåŒºå—é“¾é¢†åŸŸç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†æä¾›çš„è‹±æ–‡æ–‡ç« ç¿»è¯‘ä¸ºè‡ªç„¶æµç•…çš„ä¸­æ–‡ï¼Œè¦æ±‚ï¼š

1. ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ï¼ˆå¦‚DeFiã€NFTã€åŒºå—é“¾ç­‰ï¼‰
2. ç¿»è¯‘è¦ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯
3. ä¿ç•™åŸæ–‡çš„ç»“æ„å’Œè¯­è°ƒ
4. å¯¹äºä¸“ä¸šæœ¯è¯­ï¼Œé¦–æ¬¡å‡ºç°æ—¶å¯ä»¥ä¿ç•™è‹±æ–‡åŸæ–‡åœ¨æ‹¬å·ä¸­
5. æ•°å­—ã€æ—¶é—´ã€ä»·æ ¼ç­‰ä¿¡æ¯ä¿æŒåŸæ ·
6. ä¿æŒæ®µè½ç»“æ„

è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘åçš„ä¸­æ–‡å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜ã€‚"""

            user_prompt = f"""è¯·ç¿»è¯‘ä»¥ä¸‹å…³äºåŠ å¯†è´§å¸/åŒºå—é“¾çš„è‹±æ–‡æ–‡ç« ï¼š

æ ‡é¢˜ï¼š{title}

å†…å®¹ï¼š
{english_content}"""

            print("ğŸ”„ æ­£åœ¨ç¿»è¯‘ä¸ºä¸­æ–‡...")
            
            completion = self.client.chat_completions_create(
                model=GLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=TRANSLATION_TEMPERATURE,
                max_tokens=MAX_TOKENS_TRANSLATION
            )
            
            chinese_content = extract_content_from_response(completion, "å†…å®¹ç¿»è¯‘")
            if chinese_content:
                print(f"âœ… ç¿»è¯‘å®Œæˆï¼Œé•¿åº¦: {len(chinese_content)} å­—ç¬¦")
                return chinese_content
            else:
                print("âŒ æ— æ³•ä»å“åº”ä¸­æå–ç¿»è¯‘å†…å®¹")
                return None
            
        except Exception as e:
            print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
            return None
    
    def generate_summary(self, chinese_content: str, title: str = "") -> Optional[str]:
        """ç”Ÿæˆä¸­æ–‡æ–‡ç« æ‘˜è¦"""
        if not self.client:
            print("âŒ æ‘˜è¦ç”Ÿæˆå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        
        try:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸å†…å®¹åˆ†æå¸ˆã€‚è¯·ä¸ºæä¾›çš„ä¸­æ–‡æ–‡ç« ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ‘˜è¦ï¼Œè¦æ±‚ï¼š

1. æ‘˜è¦é•¿åº¦æ§åˆ¶åœ¨200-300å­—
2. çªå‡ºæ–‡ç« çš„æ ¸å¿ƒè¦ç‚¹å’Œå…³é”®ä¿¡æ¯
3. ä¿æŒå®¢è§‚ä¸­æ€§çš„è¯­è°ƒ
4. åŒ…å«é‡è¦çš„æ•°æ®ã€äº‹ä»¶æˆ–è¶‹åŠ¿
5. é€‚åˆä½œä¸ºæ–‡ç« çš„å¼€å¤´æ®µè½
6. è¯­è¨€è¦ä¸“ä¸šä¸”æ˜“æ‡‚

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ "æ‘˜è¦ï¼š"ç­‰æ ‡é¢˜ã€‚"""

            user_prompt = f"""è¯·ä¸ºä»¥ä¸‹ä¸­æ–‡æ–‡ç« ç”Ÿæˆæ‘˜è¦ï¼š

æ ‡é¢˜ï¼š{title}

æ–‡ç« å†…å®¹ï¼š
{chinese_content}"""

            print("ğŸ“ æ­£åœ¨ç”Ÿæˆæ–‡ç« æ‘˜è¦...")
            
            completion = self.client.chat_completions_create(
                model=GLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=SUMMARY_TEMPERATURE,
                max_tokens=MAX_TOKENS_SUMMARY
            )
            
            summary = extract_content_from_response(completion, "æ‘˜è¦ç”Ÿæˆ")
            if summary:
                print(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")
                return summary
            else:
                print("âŒ æ— æ³•ä»å“åº”ä¸­æå–æ‘˜è¦å†…å®¹")
                return None
            
        except Exception as e:
            print(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def translate_title(self, english_title: str, max_retries: int = 2) -> str:
        """
        ç¿»è¯‘æ–‡ç« æ ‡é¢˜ï¼ˆå¸¦é‡è¯•æœºåˆ¶å’Œfallbackï¼‰
        è¿”å›ç¿»è¯‘åçš„æ ‡é¢˜ï¼Œå¦‚æœç¿»è¯‘å¤±è´¥åˆ™è¿”å›åŸæ ‡é¢˜
        """
        if not english_title or not english_title.strip():
            return "LookOnChain é“¾ä¸Šæ•°æ®åˆ†æ"
        
        original_title = english_title.strip()
        
        if not self.client:
            print("âš ï¸ æ ‡é¢˜ç¿»è¯‘å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨åŸæ ‡é¢˜")
            return original_title
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸å†…å®¹ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†è‹±æ–‡æ ‡é¢˜ç¿»è¯‘ä¸ºå¸å¼•äººçš„ä¸­æ–‡æ ‡é¢˜ï¼Œè¦æ±‚ï¼š

1. ä¿æŒåŸæ„å‡†ç¡®
2. ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯
3. é€‚åˆä½œä¸ºæ–°é—»æˆ–åˆ†ææ–‡ç« æ ‡é¢˜
4. é•¿åº¦é€‚ä¸­ï¼ˆ20-50å­—ï¼‰
5. ä¿ç•™å…³é”®æœ¯è¯­çš„ä¸“ä¸šæ€§

è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘åçš„ä¸­æ–‡æ ‡é¢˜ã€‚"""

        user_prompt = f"è¯·ç¿»è¯‘ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ï¼š{original_title}"

        for attempt in range(max_retries + 1):
            try:
                print(f"ğŸ·ï¸ æ­£åœ¨ç¿»è¯‘æ ‡é¢˜... (å°è¯• {attempt + 1}/{max_retries + 1})")
                
                completion = self.client.chat_completions_create(
                    model=GLM_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=TRANSLATION_TEMPERATURE,
                    max_tokens=200
                )
                
                # ä½¿ç”¨ç¨³å¥çš„å†…å®¹æå–å‡½æ•°
                chinese_title = extract_content_from_response(
                    completion, 
                    f"æ ‡é¢˜ç¿»è¯‘-å°è¯•{attempt + 1}"
                )
                
                if chinese_title:
                    # æ¸…ç†å¯èƒ½çš„å¼•å·å’Œå¤šä½™ç©ºæ ¼
                    chinese_title = chinese_title.strip().strip('"\'').strip()
                    
                    if chinese_title:  # ç¡®ä¿æ¸…ç†åä¸ä¸ºç©º
                        print(f"âœ… æ ‡é¢˜ç¿»è¯‘å®Œæˆ: {chinese_title}")
                        return chinese_title
                
                # å¦‚æœè¿™æ¬¡å°è¯•å¤±è´¥ï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_retries:
                    print(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡ç¿»è¯‘æ— æ•ˆï¼Œç­‰å¾…åé‡è¯•...")
                    time.sleep(2)
                    
            except Exception as e:
                print(f"âŒ ç¬¬ {attempt + 1} æ¬¡æ ‡é¢˜ç¿»è¯‘å‡ºé”™: {e}")
                if attempt < max_retries:
                    print(f"â³ ç­‰å¾… 2 ç§’åé‡è¯•...")
                    time.sleep(2)
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸæ ‡é¢˜ä½œä¸ºfallback
        print(f"âš ï¸ æ ‡é¢˜ç¿»è¯‘å…¨éƒ¨å¤±è´¥ï¼Œä½¿ç”¨åŸæ ‡é¢˜: {original_title}")
        return original_title
    
    def process_article(self, article_data: Dict[str, str]) -> Optional[Dict[str, str]]:
        """
        å¤„ç†å®Œæ•´æ–‡ç« ï¼šç¿»è¯‘æ ‡é¢˜ã€å†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦
        å³ä½¿æŸäº›æ­¥éª¤å¤±è´¥ï¼Œä¹Ÿå°½å¯èƒ½ç”Ÿæˆå¯ç”¨çš„æ–‡ç« 
        """
        original_title = article_data.get('title', 'Untitled')
        print(f"\nğŸ”„ å¼€å§‹å¤„ç†æ–‡ç« : {original_title[:50]}...")
        
        # è®°å½•å¤„ç†ç»“æœ
        processing_stats = {
            'title_translation': False,
            'content_translation': False,
            'summary_generation': False
        }
        
        # ç¬¬1æ­¥ï¼šç¿»è¯‘æ ‡é¢˜ï¼ˆç°åœ¨æ°¸è¿œæœ‰fallbackï¼Œä¸ä¼šå¤±è´¥ï¼‰
        print("ğŸ“‹ æ­¥éª¤1: ç¿»è¯‘æ ‡é¢˜")
        chinese_title = self.translate_title(original_title)
        processing_stats['title_translation'] = (chinese_title != original_title)
        
        # ç­‰å¾…é¿å…APIé™åˆ¶
        time.sleep(1)
        
        # ç¬¬2æ­¥ï¼šç¿»è¯‘å†…å®¹
        print("ğŸ“„ æ­¥éª¤2: ç¿»è¯‘å†…å®¹")
        chinese_content = self.translate_to_chinese(
            article_data.get('content', ''), 
            original_title
        )
        
        if not chinese_content:
            print("âš ï¸ å†…å®¹ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")
            chinese_content = article_data.get('content', 'æœªèƒ½è·å–æ–‡ç« å†…å®¹')
        else:
            processing_stats['content_translation'] = True
        
        # ç­‰å¾…é¿å…APIé™åˆ¶
        time.sleep(1)
        
        # ç¬¬3æ­¥ï¼šç”Ÿæˆæ‘˜è¦ï¼ˆæœ‰å¤šé‡fallbackç­–ç•¥ï¼‰
        print("ğŸ“ æ­¥éª¤3: ç”Ÿæˆæ‘˜è¦")
        summary = self.generate_summary(chinese_content, chinese_title)
        
        if not summary:
            print("âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨fallbackç­–ç•¥")
            # å°è¯•ä½¿ç”¨åŸå§‹æ‘˜è¦
            original_summary = article_data.get('summary', '').strip()
            if original_summary:
                print("ğŸ“‹ ä½¿ç”¨åŸå§‹è‹±æ–‡æ‘˜è¦ä½œä¸ºfallback")
                summary = original_summary
            else:
                # ç”ŸæˆåŸºç¡€æ‘˜è¦
                print("ğŸ”§ ç”ŸæˆåŸºç¡€æ‘˜è¦")
                summary = f"æœ¬æ–‡åˆ†æäº†{chinese_title}ç›¸å…³çš„é“¾ä¸Šæ•°æ®å’Œå¸‚åœºåŠ¨æ€ï¼Œæä¾›äº†é‡è¦çš„å¸‚åœºæ´å¯Ÿã€‚"
        else:
            processing_stats['summary_generation'] = True
        
        # æ„å»ºæœ€ç»ˆæ–‡ç« æ•°æ®
        processed_article = {
            'original_title': original_title,
            'chinese_title': chinese_title,
            'original_content': article_data.get('content', ''),
            'chinese_content': chinese_content,
            'summary': summary,
            'url': article_data.get('url', ''),
            'id': article_data.get('id', ''),
            'original_summary': article_data.get('summary', ''),
            'processing_stats': processing_stats
        }
        
        # è¾“å‡ºå¤„ç†ç»“æœç»Ÿè®¡
        successful_steps = sum(processing_stats.values())
        total_steps = len(processing_stats)
        
        if successful_steps == total_steps:
            print(f"âœ… æ–‡ç« å®Œå…¨å¤„ç†æˆåŠŸ: {chinese_title}")
        elif successful_steps > 0:
            print(f"âš ï¸ æ–‡ç« éƒ¨åˆ†å¤„ç†æˆåŠŸ ({successful_steps}/{total_steps}): {chinese_title}")
        else:
            print(f"âš ï¸ æ–‡ç« åŸºæœ¬å¤„ç†å®Œæˆï¼ˆä½¿ç”¨fallbackå†…å®¹ï¼‰: {chinese_title}")
        
        return processed_article
    
    def get_api_usage_stats(self) -> Dict[str, any]:
        """è·å–APIä½¿ç”¨ç»Ÿè®¡"""
        if self.logger:
            return self.logger.get_daily_stats()
        return {"error": "æ—¥å¿—è®°å½•å™¨æœªåˆå§‹åŒ–"}