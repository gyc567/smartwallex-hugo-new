#!/usr/bin/env python3
"""
LookOnChain å†å²æ–‡ç« ç®¡ç†æ¨¡å—
è´Ÿè´£å­˜å‚¨å’Œç®¡ç†å·²å¤„ç†çš„æ–‡ç« ï¼Œç¡®ä¿å†…å®¹å»é‡
"""

import os
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional, Set
from .config import LOOKONCHAIN_HISTORY_FILE, DATA_DIR


class ArticleHistoryManager:
    """å†å²æ–‡ç« ç®¡ç†å™¨"""
    
    def __init__(self, history_file: str = None):
        self.history_file = history_file or LOOKONCHAIN_HISTORY_FILE
        self.articles_data = self._load_history()
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.history_file), exist_ok=True)
    
    def _load_history(self) -> Dict:
        """åŠ è½½å†å²æ•°æ®"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½å†å²æ•°æ®ï¼ŒåŒ…å« {len(data.get('articles', []))} ç¯‡æ–‡ç« ")
                return data
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")
                return self._create_empty_history()
        else:
            print("ğŸ“ åˆ›å»ºæ–°çš„å†å²æ•°æ®æ–‡ä»¶")
            return self._create_empty_history()
    
    def _create_empty_history(self) -> Dict:
        """åˆ›å»ºç©ºçš„å†å²æ•°æ®ç»“æ„"""
        return {
            "last_updated": datetime.now().isoformat(),
            "total_articles": 0,
            "articles": []
        }
    
    def _save_history(self):
        """ä¿å­˜å†å²æ•°æ®"""
        try:
            self.articles_data["last_updated"] = datetime.now().isoformat()
            self.articles_data["total_articles"] = len(self.articles_data["articles"])
            
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.articles_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æˆåŠŸä¿å­˜å†å²æ•°æ®ï¼Œå…± {self.articles_data['total_articles']} ç¯‡æ–‡ç« ")
        except Exception as e:
            print(f"âŒ ä¿å­˜å†å²æ•°æ®å¤±è´¥: {e}")
    
    def _generate_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        # æ¸…ç†å†…å®¹ï¼šç§»é™¤å¤šä½™çš„ç©ºç™½å’Œæ ‡ç‚¹
        import re
        cleaned_content = re.sub(r'\s+', ' ', content.strip())
        # ç§»é™¤å¸¸è§çš„æ ‡ç‚¹ç¬¦å·å·®å¼‚
        cleaned_content = re.sub(r'[^\w\s\u4e00-\u9fff]', '', cleaned_content)
        # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
        cleaned_content = cleaned_content.lower()
        
        return hashlib.md5(cleaned_content.encode('utf-8')).hexdigest()
    
    def _generate_title_hash(self, title: str) -> str:
        """ç”Ÿæˆæ ‡é¢˜å“ˆå¸Œå€¼"""
        # æ¸…ç†æ ‡é¢˜
        import re
        cleaned_title = re.sub(r'\s+', ' ', title.strip())
        cleaned_title = re.sub(r'[^\w\s\u4e00-\u9fff]', '', cleaned_title)
        cleaned_title = cleaned_title.lower()
        
        return hashlib.md5(cleaned_title.encode('utf-8')).hexdigest()
    
    def is_duplicate(self, article: Dict) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦é‡å¤"""
        title_hash = self._generate_title_hash(article.get('title', ''))
        content_hash = self._generate_content_hash(article.get('content', ''))
        
        for existing_article in self.articles_data["articles"]:
            existing_title_hash = existing_article.get('title_hash')
            existing_content_hash = existing_article.get('content_hash')
            
            # æ ‡é¢˜å®Œå…¨ç›¸åŒè®¤ä¸ºæ˜¯é‡å¤
            if title_hash == existing_title_hash:
                print(f"ğŸ” å‘ç°é‡å¤æ ‡é¢˜: {article.get('title', 'N/A')}")
                return True
            
            # å†…å®¹ç›¸ä¼¼åº¦è¶…è¿‡90%è®¤ä¸ºæ˜¯é‡å¤
            if content_hash == existing_content_hash:
                print(f"ğŸ” å‘ç°é‡å¤å†…å®¹: {article.get('title', 'N/A')}")
                return True
        
        return False
    
    def add_article(self, article: Dict) -> bool:
        """æ·»åŠ æ–‡ç« åˆ°å†å²è®°å½•"""
        if self.is_duplicate(article):
            return False
        
        article_record = {
            "title": article.get('title', ''),
            "url": article.get('url', ''),
            "title_hash": self._generate_title_hash(article.get('title', '')),
            "content_hash": self._generate_content_hash(article.get('content', '')),
            "processed_date": datetime.now().isoformat(),
            "content_length": len(article.get('content', ''))
        }
        
        self.articles_data["articles"].append(article_record)
        self._save_history()
        
        print(f"ğŸ“ æ·»åŠ æ–‡ç« åˆ°å†å²è®°å½•: {article.get('title', 'N/A')[:50]}...")
        return True
    
    def add_articles_batch(self, articles: List[Dict]) -> List[Dict]:
        """æ‰¹é‡æ·»åŠ æ–‡ç« ï¼Œè¿”å›éé‡å¤çš„æ–‡ç« """
        unique_articles = []
        
        for article in articles:
            if not self.is_duplicate(article):
                self.add_article(article)
                unique_articles.append(article)
            else:
                print(f"âš ï¸ è·³è¿‡é‡å¤æ–‡ç« : {article.get('title', 'N/A')[:50]}...")
        
        print(f"ğŸ“Š æ‰¹é‡æ·»åŠ å®Œæˆ: {len(unique_articles)}/{len(articles)} ç¯‡æ–‡ç« ä¸ºæ–°æ–‡ç« ")
        return unique_articles
    
    def get_recent_articles(self, days: int = 7) -> List[Dict]:
        """è·å–æœ€è¿‘Nå¤©çš„æ–‡ç« """
        cutoff_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        cutoff_date = cutoff_date.replace(day=cutoff_date.day - days)
        
        recent_articles = []
        for article in self.articles_data["articles"]:
            processed_date = datetime.fromisoformat(article["processed_date"])
            if processed_date >= cutoff_date:
                recent_articles.append(article)
        
        return recent_articles
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_articles = len(self.articles_data["articles"])
        
        # è®¡ç®—æœ€è¿‘7å¤©çš„æ–‡ç« æ•°é‡
        recent_articles = self.get_recent_articles(7)
        
        # è®¡ç®—å†…å®¹é•¿åº¦ç»Ÿè®¡
        content_lengths = [article["content_length"] for article in self.articles_data["articles"]]
        avg_length = sum(content_lengths) / len(content_lengths) if content_lengths else 0
        
        return {
            "total_articles": total_articles,
            "recent_7_days": len(recent_articles),
            "average_content_length": round(avg_length, 0),
            "last_updated": self.articles_data["last_updated"]
        }
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_statistics()
        
        print("ğŸ“Š LookOnChain å†å²æ–‡ç« ç»Ÿè®¡")
        print("=" * 50)
        print(f"ğŸ“š æ€»æ–‡ç« æ•°: {stats['total_articles']}")
        print(f"ğŸ“… æœ€è¿‘7å¤©: {stats['recent_7_days']} ç¯‡")
        print(f"ğŸ“ å¹³å‡å†…å®¹é•¿åº¦: {stats['average_content_length']} å­—ç¬¦")
        print(f"ğŸ• æœ€åæ›´æ–°: {stats['last_updated']}")
        print("=" * 50)
    
    def clear_old_articles(self, days: int = 30):
        """æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§æ–‡ç« """
        cutoff_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        from datetime import timedelta
        cutoff_date = cutoff_date - timedelta(days=days)
        
        original_count = len(self.articles_data["articles"])
        
        self.articles_data["articles"] = [
            article for article in self.articles_data["articles"]
            if datetime.fromisoformat(article["processed_date"]) >= cutoff_date
        ]
        
        removed_count = original_count - len(self.articles_data["articles"])
        
        if removed_count > 0:
            self._save_history()
            print(f"ğŸ§¹ å·²æ¸…ç† {removed_count} ç¯‡è¶…è¿‡ {days} å¤©çš„æ—§æ–‡ç« ")
        else:
            print(f"ğŸ“ æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ–‡ç« ")