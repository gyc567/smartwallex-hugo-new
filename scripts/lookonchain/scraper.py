"""
LookOnChain ç½‘ç«™çˆ¬è™«æ¨¡å—
è´Ÿè´£ä» LookOnChain è·å–å‰3ç¯‡æ–‡ç« å†…å®¹
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import hashlib
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
from .config import (
    LOOKONCHAIN_BASE_URL, LOOKONCHAIN_FEEDS_URL, USER_AGENT, REQUEST_TIMEOUT, 
    MAX_RETRIES, RETRY_DELAY, ARTICLE_MIN_LENGTH, ARTICLE_MAX_LENGTH
)


class LookOnChainScraper:
    """LookOnChain æ–‡ç« çˆ¬è™«ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_feeds_page(self) -> Optional[BeautifulSoup]:
        """è·å– LookOnChain é¦–é¡µå†…å®¹"""
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸ” æ­£åœ¨è·å– LookOnChain é¦–é¡µå†…å®¹... (å°è¯• {attempt + 1}/{MAX_RETRIES})")
                response = self.session.get(LOOKONCHAIN_FEEDS_URL, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                print("âœ… æˆåŠŸè·å–é¦–é¡µå†…å®¹")
                return soup
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è·å–é¦–é¡µå¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    print("âŒ æ— æ³•è·å– LookOnChain é¦–é¡µ")
                    return None
    
    def extract_article_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """ä»é¦–é¡µæå–æ–‡ç« é“¾æ¥å’ŒåŸºæœ¬ä¿¡æ¯"""
        articles = []
        
        try:
            # æ ¹æ®å®é™…åˆ†æçš„ LookOnChain ç½‘ç«™ç»“æ„æå–æ–‡ç« 
            # é¦–å…ˆå°è¯•ä¸»è¦çš„æ–‡ç« åˆ—è¡¨å®¹å™¨
            feeds_container = soup.select_one('#index_feeds_list')
            if feeds_container:
                print("ğŸ“° æ‰¾åˆ°ä¸»è¦æ–‡ç« åˆ—è¡¨å®¹å™¨ #index_feeds_list")
                article_elements = feeds_container.select('a[href*="/articles/"]')
            else:
                # å¤‡é€‰æ–¹æ¡ˆï¼šç›´æ¥æŸ¥æ‰¾æ–‡ç« é“¾æ¥
                article_elements = soup.select('a[href*="/articles/"]')
            
            if article_elements:
                print(f"ğŸ“° æ‰¾åˆ° {len(article_elements)} ä¸ªæ–‡ç« é“¾æ¥")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ç« é“¾æ¥ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                fallback_selectors = [
                    'a[href*="article"]',  # åŒ…å«articleçš„é“¾æ¥
                    'a[href*="post"]',     # åŒ…å«postçš„é“¾æ¥
                    'article a',           # åœ¨articleæ ‡ç­¾å†…çš„é“¾æ¥
                    '.post a', '.article a', '.news-item a',  # åœ¨å¸¸è§æ–‡ç« classå†…çš„é“¾æ¥
                ]
                
                for selector in fallback_selectors:
                    article_elements = soup.select(selector)
                    if article_elements:
                        print(f"ğŸ“° é€šè¿‡å¤‡é€‰æ–¹æ¡ˆæ‰¾åˆ° {len(article_elements)} ä¸ªæ–‡ç« å…ƒç´  (é€‰æ‹©å™¨: {selector})")
                        break
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥ï¼Œå°†åˆ†æé¡µé¢ç»“æ„")
                    # è°ƒè¯•ï¼šè¾“å‡ºé¡µé¢çš„ä¸»è¦é“¾æ¥
                    all_links = soup.select('a[href]')[:10]
                    print("ğŸ” é¡µé¢å‰10ä¸ªé“¾æ¥:")
                    for i, link in enumerate(all_links, 1):
                        href = link.get('href', '')
                        text = link.get_text().strip()[:50]
                        print(f"   {i}. {href} - {text}")
                    article_elements = []
            
            for element in article_elements[:5]:  # åªå¤„ç†å‰5ä¸ªå…ƒç´ ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå€™é€‰
                try:
                    # ç¡®ä¿elementæ˜¯é“¾æ¥å…ƒç´ 
                    link_elem = element if element.name == 'a' else element.find('a')
                    if not link_elem:
                        continue
                    
                    href = link_elem.get('href')
                    if not href:
                        continue
                    
                    # ç¡®ä¿æ˜¯å®Œæ•´URL
                    full_url = urljoin(LOOKONCHAIN_BASE_URL, href)
                    
                    # æå–æ ‡é¢˜ - æ ¹æ®LookOnChainç½‘ç«™ç»“æ„ï¼Œæ ‡é¢˜é€šå¸¸åœ¨é“¾æ¥æ–‡æœ¬ä¸­
                    title = link_elem.get_text().strip()
                    
                    # å¦‚æœæ ‡é¢˜ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                    if not title or len(title) < 10:
                        # å°è¯•æŸ¥æ‰¾çˆ¶å®¹å™¨ä¸­çš„æ ‡é¢˜å…ƒç´ 
                        parent = link_elem.parent
                        if parent:
                            title_candidates = parent.select('.title, [class*="title"], h1, h2, h3, h4')
                            for candidate in title_candidates:
                                candidate_text = candidate.get_text().strip()
                                if candidate_text and len(candidate_text) > 10:
                                    title = candidate_text
                                    break
                    
                    # æ¸…ç†æ ‡é¢˜ä¸­çš„æ—¥æœŸç­‰é¢å¤–ä¿¡æ¯
                    if title:
                        # ç§»é™¤æ—¥æœŸæ ¼å¼å¦‚ "2025.01.22"
                        import re
                        title = re.sub(r'\d{4}\.\d{2}\.\d{2}', '', title).strip()
                        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                        title = ' '.join(title.split())
                    
                    # æå–æ‘˜è¦ - åœ¨LookOnChainä¸­å¯èƒ½ä¸å®¹æ˜“è·å–ï¼Œå…ˆè®¾ä¸ºç©º
                    summary = ''
                    
                    # éªŒè¯æ–‡ç« URLæ ¼å¼
                    if '/articles/' in href and title and len(title) > 10:
                        article_info = {
                            'title': title,
                            'url': full_url,
                            'summary': summary,
                            'id': hashlib.md5(full_url.encode()).hexdigest()[:12]
                        }
                        articles.append(article_info)
                        print(f"âœ… æå–æ–‡ç« : {title[:50]}...")
                        
                except Exception as e:
                    print(f"âš ï¸ è§£ææ–‡ç« å…ƒç´ å¤±è´¥: {e}")
                    continue
            
            print(f"ğŸ“‹ æˆåŠŸæå– {len(articles)} ä¸ªæ–‡ç« é“¾æ¥")
            return articles[:3]  # åªè¿”å›å‰3ç¯‡
            
        except Exception as e:
            print(f"âŒ æå–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def get_article_content(self, article_url: str) -> Optional[str]:
        """è·å–å•ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹"""
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸ“– æ­£åœ¨è·å–æ–‡ç« å†…å®¹: {article_url}")
                response = self.session.get(article_url, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', '.ads', '.advertisement']):
                    element.decompose()
                
                # å°è¯•å¤šç§æ–¹å¼æå–æ–‡ç« ä¸»ä½“å†…å®¹
                content_selectors = [
                    'article', '.article-content', '.post-content', '.content',
                    '.main-content', '[class*="content"]', '.entry-content',
                    'main', '[role="main"]'
                ]
                
                content = ''
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(separator=' ', strip=True)
                        break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œå°è¯•æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
                if not content or len(content) < ARTICLE_MIN_LENGTH:
                    # å°è¯•ç§»é™¤å¯¼èˆªã€é¡µè„šã€ä¾§è¾¹æ ç­‰å…ƒç´ åå†æå–
                    for elem in soup.select('nav, footer, aside, header, .nav, .footer, .sidebar, .menu'):
                        if elem:
                            elem.decompose()
                    
                    # å°è¯•æ›´ç²¾ç¡®çš„å†…å®¹é€‰æ‹©å™¨
                    precise_selectors = [
                        'main article', 'div[class*="post"]', 'div[class*="article"]',
                        '.post-body', '.article-body', '.entry', '.post'
                    ]
                    
                    for selector in precise_selectors:
                        content_elem = soup.select_one(selector)
                        if content_elem:
                            content = content_elem.get_text(separator=' ', strip=True)
                            if len(content) >= ARTICLE_MIN_LENGTH:
                                break
                    
                    # æœ€åå°è¯•bodyï¼Œä½†è¿‡æ»¤æ‰æ˜æ˜¾çš„å¯¼èˆªæ–‡æœ¬
                    if not content or len(content) < ARTICLE_MIN_LENGTH:
                        body = soup.find('body')
                        if body:
                            raw_content = body.get_text(separator=' ', strip=True)
                            # ç®€å•è¿‡æ»¤ï¼šç§»é™¤å¸¸è§çš„å¯¼èˆªæ–‡æœ¬æ¨¡å¼
                            import re
                            # ç§»é™¤èœå•ã€å¯¼èˆªç›¸å…³æ–‡æœ¬
                            filtered_content = re.sub(r'\b(Home|Login|Register|About|Contact|Menu|Navigation|Footer|Header|Search|Subscribe|Follow|Tweet|Share|Like|Reply|Retweet)\b', '', raw_content, flags=re.IGNORECASE)
                            # ç§»é™¤å¸¸è§çš„æ— ç”¨è¯æ±‡
                            filtered_content = re.sub(r'\b(trending|popular|latest|more|read more|continue reading|click here|learn more)\b', '', filtered_content, flags=re.IGNORECASE)
                            # ç§»é™¤å¤šä½™ç©ºæ ¼
                            filtered_content = ' '.join(filtered_content.split())
                            
                            if len(filtered_content) >= ARTICLE_MIN_LENGTH:
                                content = filtered_content
                
                # å†…å®¹é•¿åº¦æ£€æŸ¥
                if len(content) < ARTICLE_MIN_LENGTH:
                    print(f"âš ï¸ æ–‡ç« å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦")
                    return None
                
                if len(content) > ARTICLE_MAX_LENGTH:
                    content = content[:ARTICLE_MAX_LENGTH] + "..."
                    print(f"ğŸ“ æ–‡ç« å†…å®¹å·²æˆªæ–­è‡³ {ARTICLE_MAX_LENGTH} å­—ç¬¦")
                
                print(f"âœ… æˆåŠŸè·å–æ–‡ç« å†…å®¹: {len(content)} å­—ç¬¦")
                return content
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è·å–æ–‡ç« å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    print(f"âŒ æ— æ³•è·å–æ–‡ç« : {article_url}")
                    return None
    
    def scrape_top_articles(self) -> List[Dict[str, str]]:
        """çˆ¬å–å‰3ç¯‡æ–‡ç« çš„å®Œæ•´ä¿¡æ¯"""
        print("ğŸš€ å¼€å§‹çˆ¬å– LookOnChain æ–‡ç« ...")
        
        # è·å–é¦–é¡µ
        soup = self.get_feeds_page()
        if not soup:
            return []
        
        # æå–æ–‡ç« é“¾æ¥
        article_links = self.extract_article_links(soup)
        if not article_links:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
            return []
        
        print(f"ğŸ“š å‡†å¤‡è·å– {len(article_links)} ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹")
        
        # è·å–æ¯ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹
        complete_articles = []
        for i, article_info in enumerate(article_links, 1):
            print(f"\nğŸ“° å¤„ç†æ–‡ç«  {i}: {article_info['title'][:50]}...")
            
            content = self.get_article_content(article_info['url'])
            if content:
                article_info['content'] = content
                complete_articles.append(article_info)
                print(f"âœ… æ–‡ç«  {i} å¤„ç†å®Œæˆ")
            else:
                print(f"âŒ æ–‡ç«  {i} å†…å®¹è·å–å¤±è´¥")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if i < len(article_links):
                time.sleep(1)
        
        print(f"\nğŸ‰ æˆåŠŸçˆ¬å– {len(complete_articles)} ç¯‡å®Œæ•´æ–‡ç« ")
        return complete_articles
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()