"""
LookOnChain ç½‘ç«™çˆ¬è™«æ¨¡å—
è´Ÿè´£ä» LookOnChain è·å–å‰3ç¯‡æ–‡ç« å†…å®¹
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import hashlib
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
from .config import (
    LOOKONCHAIN_FEEDS_URL, USER_AGENT, REQUEST_TIMEOUT, 
    MAX_RETRIES, RETRY_DELAY, ARTICLE_MIN_LENGTH, ARTICLE_MAX_LENGTH
)


class LookOnChainScraper:
    """LookOnChain æ–‡ç« çˆ¬è™«ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_feeds_page(self) -> Optional[BeautifulSoup]:
        """è·å– LookOnChain é¦–é¡µå†…å®¹"""
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸ” æ­£åœ¨è·å– LookOnChain é¦–é¡µå†…å®¹... (å°è¯• {attempt + 1}/{MAX_RETRIES})")
                response = self.session.get(LOOKONCHAIN_FEEDS_URL, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                print("âœ… æˆåŠŸè·å–é¦–é¡µå†…å®¹")
                return soup
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è·å–é¦–é¡µå¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    print("âŒ æ— æ³•è·å– LookOnChain é¦–é¡µ")
                    return None
    
    def extract_article_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """ä»é¦–é¡µæå–æ–‡ç« é“¾æ¥å’ŒåŸºæœ¬ä¿¡æ¯"""
        articles = []
        
        try:
            # æ ¹æ® LookOnChain ç½‘ç«™ç»“æ„æå–æ–‡ç« 
            # è¿™é‡Œéœ€è¦åˆ†æç½‘ç«™çš„å®é™…DOMç»“æ„æ¥ç¡®å®šé€‰æ‹©å™¨
            article_selectors = [
                'article',  # æ ‡å‡†æ–‡ç« æ ‡ç­¾
                '.post', '.article', '.news-item',  # å¸¸è§çš„æ–‡ç« class
                '[class*="feed"], [class*="item"], [class*="post"]'  # åŒ…å«ç›¸å…³å…³é”®è¯çš„class
            ]
            
            for selector in article_selectors:
                article_elements = soup.select(selector)
                if article_elements:
                    print(f"ğŸ“° æ‰¾åˆ° {len(article_elements)} ä¸ªæ–‡ç« å…ƒç´  (é€‰æ‹©å™¨: {selector})")
                    break
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†ç»“æ„ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«é“¾æ¥çš„å…ƒç´ 
                article_elements = soup.select('a[href*="article"], a[href*="post"], a[href*="news"]')
                if not article_elements:
                    # æœ€åå°è¯•ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ–‡æœ¬å†…å®¹çš„é“¾æ¥
                    article_elements = soup.select('a[href]')
                    article_elements = [elem for elem in article_elements if elem.get_text().strip()][:10]
            
            for element in article_elements[:5]:  # åªå¤„ç†å‰5ä¸ªå…ƒç´ ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå€™é€‰
                try:
                    # æå–é“¾æ¥
                    link_elem = element if element.name == 'a' else element.find('a')
                    if not link_elem:
                        continue
                    
                    href = link_elem.get('href')
                    if not href:
                        continue
                    
                    # ç¡®ä¿æ˜¯å®Œæ•´URL
                    full_url = urljoin(LOOKONCHAIN_FEEDS_URL, href)
                    
                    # æå–æ ‡é¢˜
                    title = ''
                    title_elem = element.find(['h1', 'h2', 'h3', 'h4', '.title', '[class*="title"]'])
                    if title_elem:
                        title = title_elem.get_text().strip()
                    else:
                        title = link_elem.get_text().strip()
                    
                    # æå–æ‘˜è¦
                    summary = ''
                    summary_elem = element.find(['.summary', '.excerpt', '.description', 'p'])
                    if summary_elem:
                        summary = summary_elem.get_text().strip()[:200]
                    
                    if title and len(title) > 10:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                        article_info = {
                            'title': title,
                            'url': full_url,
                            'summary': summary,
                            'id': hashlib.md5(full_url.encode()).hexdigest()[:12]
                        }
                        articles.append(article_info)
                        
                except Exception as e:
                    print(f"âš ï¸ è§£ææ–‡ç« å…ƒç´ å¤±è´¥: {e}")
                    continue
            
            print(f"ğŸ“‹ æˆåŠŸæå– {len(articles)} ä¸ªæ–‡ç« é“¾æ¥")
            return articles[:3]  # åªè¿”å›å‰3ç¯‡
            
        except Exception as e:
            print(f"âŒ æå–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def get_article_content(self, article_url: str) -> Optional[str]:
        """è·å–å•ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹"""
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸ“– æ­£åœ¨è·å–æ–‡ç« å†…å®¹: {article_url}")
                response = self.session.get(article_url, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', '.ads', '.advertisement']):
                    element.decompose()
                
                # å°è¯•å¤šç§æ–¹å¼æå–æ–‡ç« ä¸»ä½“å†…å®¹
                content_selectors = [
                    'article', '.article-content', '.post-content', '.content',
                    '.main-content', '[class*="content"]', '.entry-content',
                    'main', '[role="main"]'
                ]
                
                content = ''
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(separator=' ', strip=True)
                        break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œæå–bodyä¸­çš„æ–‡æœ¬
                if not content or len(content) < ARTICLE_MIN_LENGTH:
                    body = soup.find('body')
                    if body:
                        content = body.get_text(separator=' ', strip=True)
                
                # å†…å®¹é•¿åº¦æ£€æŸ¥
                if len(content) < ARTICLE_MIN_LENGTH:
                    print(f"âš ï¸ æ–‡ç« å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦")
                    return None
                
                if len(content) > ARTICLE_MAX_LENGTH:
                    content = content[:ARTICLE_MAX_LENGTH] + "..."
                    print(f"ğŸ“ æ–‡ç« å†…å®¹å·²æˆªæ–­è‡³ {ARTICLE_MAX_LENGTH} å­—ç¬¦")
                
                print(f"âœ… æˆåŠŸè·å–æ–‡ç« å†…å®¹: {len(content)} å­—ç¬¦")
                return content
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è·å–æ–‡ç« å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    print(f"âŒ æ— æ³•è·å–æ–‡ç« : {article_url}")
                    return None
    
    def scrape_top_articles(self) -> List[Dict[str, str]]:
        """çˆ¬å–å‰3ç¯‡æ–‡ç« çš„å®Œæ•´ä¿¡æ¯"""
        print("ğŸš€ å¼€å§‹çˆ¬å– LookOnChain æ–‡ç« ...")
        
        # è·å–é¦–é¡µ
        soup = self.get_feeds_page()
        if not soup:
            return []
        
        # æå–æ–‡ç« é“¾æ¥
        article_links = self.extract_article_links(soup)
        if not article_links:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
            return []
        
        print(f"ğŸ“š å‡†å¤‡è·å– {len(article_links)} ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹")
        
        # è·å–æ¯ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹
        complete_articles = []
        for i, article_info in enumerate(article_links, 1):
            print(f"\nğŸ“° å¤„ç†æ–‡ç«  {i}: {article_info['title'][:50]}...")
            
            content = self.get_article_content(article_info['url'])
            if content:
                article_info['content'] = content
                complete_articles.append(article_info)
                print(f"âœ… æ–‡ç«  {i} å¤„ç†å®Œæˆ")
            else:
                print(f"âŒ æ–‡ç«  {i} å†…å®¹è·å–å¤±è´¥")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if i < len(article_links):
                time.sleep(1)
        
        print(f"\nğŸ‰ æˆåŠŸçˆ¬å– {len(complete_articles)} ç¯‡å®Œæ•´æ–‡ç« ")
        return complete_articles
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()