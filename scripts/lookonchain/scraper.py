"""
LookOnChain ç½‘ç«™çˆ¬è™«æ¨¡å—
è´Ÿè´£ä» LookOnChain è·å–å‰3ç¯‡æ–‡ç« å†…å®¹
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import hashlib
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
from .config import (
    LOOKONCHAIN_BASE_URL, LOOKONCHAIN_FEEDS_URL, USER_AGENT, REQUEST_TIMEOUT, 
    MAX_RETRIES, RETRY_DELAY, ARTICLE_MIN_LENGTH, ARTICLE_MAX_LENGTH
)


class LookOnChainScraper:
    """LookOnChain æ–‡ç« çˆ¬è™«ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_feeds_page(self) -> Optional[BeautifulSoup]:
        """è·å– LookOnChain é¦–é¡µå†…å®¹"""
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸ” æ­£åœ¨è·å– LookOnChain é¦–é¡µå†…å®¹... (å°è¯• {attempt + 1}/{MAX_RETRIES})")
                response = self.session.get(LOOKONCHAIN_FEEDS_URL, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                print("âœ… æˆåŠŸè·å–é¦–é¡µå†…å®¹")
                return soup
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è·å–é¦–é¡µå¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    print("âŒ æ— æ³•è·å– LookOnChain é¦–é¡µ")
                    return None
    
    def extract_article_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """ä»é¦–é¡µæå–æ–‡ç« é“¾æ¥å’ŒåŸºæœ¬ä¿¡æ¯"""
        articles = []
        
        try:
            # æ ¹æ®å®é™…åˆ†æçš„ LookOnChain ç½‘ç«™ç»“æ„æå–æ–‡ç« 
            # é¦–å…ˆå°è¯•ä¸»è¦çš„æ–‡ç« åˆ—è¡¨å®¹å™¨
            feeds_container = soup.select_one('#index_feeds_list')
            if feeds_container:
                print("ğŸ“° æ‰¾åˆ°ä¸»è¦æ–‡ç« åˆ—è¡¨å®¹å™¨ #index_feeds_list")
                article_elements = feeds_container.select('a[href*="/articles/"]')
            else:
                # å¤‡é€‰æ–¹æ¡ˆï¼šç›´æ¥æŸ¥æ‰¾æ–‡ç« é“¾æ¥
                article_elements = soup.select('a[href*="/articles/"]')
            
            if article_elements:
                print(f"ğŸ“° æ‰¾åˆ° {len(article_elements)} ä¸ªæ–‡ç« é“¾æ¥")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ç« é“¾æ¥ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                fallback_selectors = [
                    'a[href*="article"]',  # åŒ…å«articleçš„é“¾æ¥
                    'a[href*="post"]',     # åŒ…å«postçš„é“¾æ¥
                    'article a',           # åœ¨articleæ ‡ç­¾å†…çš„é“¾æ¥
                    '.post a', '.article a', '.news-item a',  # åœ¨å¸¸è§æ–‡ç« classå†…çš„é“¾æ¥
                ]
                
                for selector in fallback_selectors:
                    article_elements = soup.select(selector)
                    if article_elements:
                        print(f"ğŸ“° é€šè¿‡å¤‡é€‰æ–¹æ¡ˆæ‰¾åˆ° {len(article_elements)} ä¸ªæ–‡ç« å…ƒç´  (é€‰æ‹©å™¨: {selector})")
                        break
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥ï¼Œå°†åˆ†æé¡µé¢ç»“æ„")
                    # è°ƒè¯•ï¼šè¾“å‡ºé¡µé¢çš„ä¸»è¦é“¾æ¥
                    all_links = soup.select('a[href]')[:10]
                    print("ğŸ” é¡µé¢å‰10ä¸ªé“¾æ¥:")
                    for i, link in enumerate(all_links, 1):
                        href = link.get('href', '')
                        text = link.get_text().strip()[:50]
                        print(f"   {i}. {href} - {text}")
                    article_elements = []
            
            for element in article_elements[:5]:  # åªå¤„ç†å‰5ä¸ªå…ƒç´ ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå€™é€‰
                try:
                    # ç¡®ä¿elementæ˜¯é“¾æ¥å…ƒç´ 
                    link_elem = element if element.name == 'a' else element.find('a')
                    if not link_elem:
                        continue
                    
                    href = link_elem.get('href')
                    if not href:
                        continue
                    
                    # ç¡®ä¿æ˜¯å®Œæ•´URL
                    full_url = urljoin(LOOKONCHAIN_BASE_URL, href)
                    
                    # æå–æ ‡é¢˜ - æ ¹æ®LookOnChainç½‘ç«™ç»“æ„ï¼Œæ ‡é¢˜é€šå¸¸åœ¨é“¾æ¥æ–‡æœ¬ä¸­
                    title = link_elem.get_text().strip()
                    
                    # å¦‚æœæ ‡é¢˜ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                    if not title or len(title) < 10:
                        # å°è¯•æŸ¥æ‰¾çˆ¶å®¹å™¨ä¸­çš„æ ‡é¢˜å…ƒç´ 
                        parent = link_elem.parent
                        if parent:
                            title_candidates = parent.select('.title, [class*="title"], h1, h2, h3, h4')
                            for candidate in title_candidates:
                                candidate_text = candidate.get_text().strip()
                                if candidate_text and len(candidate_text) > 10:
                                    title = candidate_text
                                    break
                    
                    # æ¸…ç†æ ‡é¢˜ä¸­çš„æ—¥æœŸç­‰é¢å¤–ä¿¡æ¯
                    if title:
                        # ç§»é™¤æ—¥æœŸæ ¼å¼å¦‚ "2025.01.22"
                        import re
                        title = re.sub(r'\d{4}\.\d{2}\.\d{2}', '', title).strip()
                        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                        title = ' '.join(title.split())
                    
                    # æå–æ‘˜è¦ - åœ¨LookOnChainä¸­å¯èƒ½ä¸å®¹æ˜“è·å–ï¼Œå…ˆè®¾ä¸ºç©º
                    summary = ''
                    
                    # éªŒè¯æ–‡ç« URLæ ¼å¼
                    if '/articles/' in href and title and len(title) > 10:
                        article_info = {
                            'title': title,
                            'url': full_url,
                            'summary': summary,
                            'id': hashlib.md5(full_url.encode()).hexdigest()[:12]
                        }
                        articles.append(article_info)
                        print(f"âœ… æå–æ–‡ç« : {title[:50]}...")
                        
                except Exception as e:
                    print(f"âš ï¸ è§£ææ–‡ç« å…ƒç´ å¤±è´¥: {e}")
                    continue
            
            print(f"ğŸ“‹ æˆåŠŸæå– {len(articles)} ä¸ªæ–‡ç« é“¾æ¥")
            return articles[:3]  # åªè¿”å›å‰3ç¯‡
            
        except Exception as e:
            print(f"âŒ æå–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def get_article_content(self, article_url: str) -> Optional[str]:
        """è·å–å•ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹"""
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸ“– æ­£åœ¨è·å–æ–‡ç« å†…å®¹: {article_url}")
                response = self.session.get(article_url, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', '.ads', '.advertisement', '.sidebar', '.menu', '.navigation']):
                    element.decompose()
                
                # é¦–å…ˆå°è¯•é’ˆå¯¹ LookOnChain çš„ç²¾ç¡®å†…å®¹æå–
                content = self._extract_lookonchain_content(soup)
                
                # å¦‚æœç²¾ç¡®æå–å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
                if not content or len(content) < ARTICLE_MIN_LENGTH:
                    content = self._extract_content_fallback(soup)
                
                # å†…å®¹é•¿åº¦æ£€æŸ¥
                if len(content) < ARTICLE_MIN_LENGTH:
                    print(f"âš ï¸ æ–‡ç« å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦")
                    return None
                
                if len(content) > ARTICLE_MAX_LENGTH:
                    content = content[:ARTICLE_MAX_LENGTH] + "..."
                    print(f"ğŸ“ æ–‡ç« å†…å®¹å·²æˆªæ–­è‡³ {ARTICLE_MAX_LENGTH} å­—ç¬¦")
                
                # æœ€ç»ˆå†…å®¹è´¨é‡æ£€æŸ¥
                quality_score = self._calculate_content_quality(content)
                if quality_score < 0.3:
                    print(f"âš ï¸ å†…å®¹è´¨é‡è¿‡ä½ (è¯„åˆ†: {quality_score:.2f})ï¼Œå¯èƒ½åŒ…å«è¿‡å¤šæ— å…³å†…å®¹")
                    return None
                
                print(f"âœ… æˆåŠŸè·å–æ–‡ç« å†…å®¹: {len(content)} å­—ç¬¦ï¼Œè´¨é‡è¯„åˆ†: {quality_score:.2f}")
                return content
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è·å–æ–‡ç« å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    print(f"âŒ æ— æ³•è·å–æ–‡ç« : {article_url}")
                    return None
    
    def _extract_lookonchain_content(self, soup: BeautifulSoup) -> str:
        """é’ˆå¯¹ LookOnChain ç½‘ç«™çš„ç²¾ç¡®å†…å®¹æå–"""
        # LookOnChain ç‰¹å®šçš„å†…å®¹é€‰æ‹©å™¨
        lookonchain_selectors = [
            '.article-content',
            '.post-content', 
            '.entry-content',
            '.content-body',
            '[class*="article-body"]',
            '[class*="post-body"]',
            'main article',
            '.main-content article',
            '#article-content',
            '#post-content'
        ]
        
        for selector in lookonchain_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤å†…å®¹åŒºåŸŸå†…çš„æ— ç”¨å…ƒç´ 
                for elem in content_elem.select('.ads, .share-buttons, .social-share, .related-posts, .comments'):
                    elem.decompose()
                
                content = content_elem.get_text(separator=' ', strip=True)
                if len(content) >= ARTICLE_MIN_LENGTH:
                    print(f"ğŸ¯ ä½¿ç”¨ LookOnChain ä¸“ç”¨é€‰æ‹©å™¨: {selector}")
                    return content
        
        return ""
    
    def _extract_content_fallback(self, soup: BeautifulSoup) -> str:
        """å¤‡ç”¨å†…å®¹æå–æ–¹æ³•"""
        # å°è¯•é€šç”¨çš„å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            'article', '.article-content', '.post-content', '.content',
            '.main-content', '[class*="content"]', '.entry-content',
            'main', '[role="main"]'
        ]
        
        content = ''
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator=' ', strip=True)
                if len(content) >= ARTICLE_MIN_LENGTH:
                    break
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿå†…å®¹ï¼Œä½¿ç”¨æ™ºèƒ½è¿‡æ»¤
        if not content or len(content) < ARTICLE_MIN_LENGTH:
            content = self._extract_with_intelligent_filtering(soup)
        
        return content
    
    def _extract_with_intelligent_filtering(self, soup: BeautifulSoup) -> str:
        """ä½¿ç”¨æ™ºèƒ½è¿‡æ»¤æå–å†…å®¹"""
        body = soup.find('body')
        if not body:
            return ""
        
        raw_content = body.get_text(separator=' ', strip=True)
        
        # æ™ºèƒ½è¿‡æ»¤æ¨¡å¼ - æ›´ç²¾ç¡®åœ°ç§»é™¤æ— å…³å†…å®¹
        import re
        
        # ç§»é™¤ LookOnChain ç‰¹æœ‰çš„æ— ç”¨æ–‡æœ¬æ¨¡å¼
        noise_patterns = [
            r'Lookonchain\s*/\s*\d{4}\.\d{2}\.\d{2}',
            r'X\s+å…³æ³¨Telegram\s+åŠ å…¥',
            r'\d{4}\.\d{2}\.\d{2}\s+\d{2}:\d{2}:\d{2}',
            r'Follow\s+us\s+on\s+(Twitter|Telegram|X)',
            r'Join\s+our\s+(community|channel|group)',
            r'Subscribe\s+to\s+our\s+(newsletter|channel)',
            r'Click\s+here\s+to\s+read\s+more',
            r'Read\s+more\s+at\s+the\s+source',
            r'Continue\s+reading',
            r'Source\s+link',
            r'Original\s+article',
            r'\b(Home|Login|Register|About|Contact|Menu|Navigation|Footer|Header|Search|Subscribe|Follow|Share|Like|Reply|Retweet|Tweet|Copy link|Download|Upload|Settings|Profile|Dashboard|Notifications|APP|åº”ç”¨å•†åº—|ç™»å½•|æ³¨å†Œ|é…ç½®æ–‡ä»¶|å®‰å…¨|æ³¨é”€|åŠ¨æ€|æ–‡ç« |æœç´¢å†å²|æ¸…é™¤å…¨éƒ¨|è¶‹åŠ¿æœç´¢|å…³æ³¨æˆ‘ä»¬|åŠ å…¥|ä¸‹è½½å›¾ç‰‡|å¤åˆ¶é“¾æ¥|ç›¸å…³å†…å®¹|åŸæ–‡|çƒ­ç‚¹æ–°é—»|æ›´å¤šçƒ­é—¨æ–‡ç« |æ›´å¤š)\b',
            r'\b(trending|popular|latest|hot|new|more|read more|continue reading|click here|learn more|show more|load more|view all|see all)\b'
        ]
        
        filtered_content = raw_content
        for pattern in noise_patterns:
            filtered_content = re.sub(pattern, ' ', filtered_content, flags=re.IGNORECASE)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        filtered_content = ' '.join(filtered_content.split())
        
        return filtered_content
    
    def _calculate_content_quality(self, content: str) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ† (0-1)"""
        if not content:
            return 0.0
        
        score = 0.0
        
        # 1. é•¿åº¦è¯„åˆ† (0-0.3)
        length_score = min(len(content) / 2000, 1.0) * 0.3
        score += length_score
        
        # 2. å¯†åº¦è¯„åˆ† - æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„å…³é”®è¯ (0-0.3)
        crypto_keywords = [
            'bitcoin', 'btc', 'ethereum', 'eth', 'crypto', 'blockchain', 
            'defi', 'nft', 'token', 'coin', 'wallet', 'address', 'transaction',
            'exchange', 'trading', 'investment', 'market', 'price', 'usd',
            'ç¾å…ƒ', 'æ¯”ç‰¹å¸', 'ä»¥å¤ªåŠ', 'åŠ å¯†', 'åŒºå—é“¾', 'ä»£å¸', 'äº¤æ˜“', 'æŠ•èµ„'
        ]
        
        content_lower = content.lower()
        keyword_count = sum(1 for keyword in crypto_keywords if keyword in content_lower)
        density_score = min(keyword_count / 10, 1.0) * 0.3
        score += density_score
        
        # 3. ç»“æ„è¯„åˆ† - æ£€æŸ¥æ®µè½ç»“æ„ (0-0.2)
        sentences = content.split('.')
        if len(sentences) > 3:
            avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
            if 10 <= avg_sentence_length <= 30:
                score += 0.2
            elif 5 <= avg_sentence_length <= 50:
                score += 0.1
        
        # 4. å¯è¯»æ€§è¯„åˆ† - æ£€æŸ¥æ— æ„ä¹‰å­—ç¬¦æ¯”ä¾‹ (0-0.2)
        meaningful_chars = sum(1 for c in content if c.isalnum() or c.isspace() or c in '.,;:!?-')
        readability_score = (meaningful_chars / len(content)) * 0.2
        score += readability_score
        
        return min(score, 1.0)
    
    def scrape_top_articles(self) -> List[Dict[str, str]]:
        """çˆ¬å–å‰3ç¯‡æ–‡ç« çš„å®Œæ•´ä¿¡æ¯"""
        print("ğŸš€ å¼€å§‹çˆ¬å– LookOnChain æ–‡ç« ...")
        
        # è·å–é¦–é¡µ
        soup = self.get_feeds_page()
        if not soup:
            return []
        
        # æå–æ–‡ç« é“¾æ¥
        article_links = self.extract_article_links(soup)
        if not article_links:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
            return []
        
        print(f"ğŸ“š å‡†å¤‡è·å– {len(article_links)} ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹")
        
        # è·å–æ¯ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹
        complete_articles = []
        for i, article_info in enumerate(article_links, 1):
            print(f"\nğŸ“° å¤„ç†æ–‡ç«  {i}: {article_info['title'][:50]}...")
            
            content = self.get_article_content(article_info['url'])
            if content:
                article_info['content'] = content
                complete_articles.append(article_info)
                print(f"âœ… æ–‡ç«  {i} å¤„ç†å®Œæˆ")
            else:
                print(f"âŒ æ–‡ç«  {i} å†…å®¹è·å–å¤±è´¥")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if i < len(article_links):
                time.sleep(1)
        
        print(f"\nğŸ‰ æˆåŠŸçˆ¬å– {len(complete_articles)} ç¯‡å®Œæ•´æ–‡ç« ")
        return complete_articles
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()