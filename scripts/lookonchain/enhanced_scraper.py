#!/usr/bin/env python3
"""
LookOnChain å¢å¼ºçˆ¬è™«æ¨¡å—
ä¸“é—¨ç”¨äºè·å–æœ€æ–°å‰ä¸‰ç¯‡æ–‡ç« å¹¶è¿›è¡Œå†…å®¹å¤„ç†
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import hashlib
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta
from .config import (
    LOOKONCHAIN_BASE_URL, LOOKONCHAIN_FEEDS_URL, USER_AGENT, REQUEST_TIMEOUT,
    MAX_RETRIES, RETRY_DELAY, ARTICLE_MIN_LENGTH, ARTICLE_MAX_LENGTH
)


class EnhancedLookOnChainScraper:
    """å¢å¼ºçš„LookOnChainçˆ¬è™«ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': LOOKONCHAIN_BASE_URL
        })
        
        # ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è¯·æ±‚
        self.cache = {}
        self.cache_timeout = 300  # 5åˆ†é’Ÿç¼“å­˜
        
        print("ğŸ•·ï¸ å¢å¼ºLookOnChainçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key not in self.cache:
            return False
        
        cache_time, _ = self.cache[cache_key]
        return (datetime.now() - cache_time).total_seconds() < self.cache_timeout
    
    def _get_cached_response(self, cache_key: str) -> Optional[requests.Response]:
        """è·å–ç¼“å­˜çš„å“åº”"""
        if self._is_cache_valid(cache_key):
            _, response = self.cache[cache_key]
            print(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜å“åº”: {cache_key}")
            return response
        return None
    
    def _cache_response(self, cache_key: str, response: requests.Response):
        """ç¼“å­˜å“åº”"""
        self.cache[cache_key] = (datetime.now(), response)
        print(f"ğŸ’¾ ç¼“å­˜å“åº”: {cache_key}")
    
    def get_latest_articles_page(self) -> Optional[BeautifulSoup]:
        """è·å–æœ€æ–°æ–‡ç« é¡µé¢"""
        cache_key = "latest_articles_page"
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_response = self._get_cached_response(cache_key)
        if cached_response:
            return BeautifulSoup(cached_response.content, 'html.parser')
        
        # æ²¡æœ‰ç¼“å­˜ï¼Œå‘èµ·è¯·æ±‚
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸ” æ­£åœ¨è·å–æœ€æ–°æ–‡ç« é¡µé¢... (å°è¯• {attempt + 1}/{MAX_RETRIES})")
                response = self.session.get(LOOKONCHAIN_FEEDS_URL, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                
                # ç¼“å­˜å“åº”
                self._cache_response(cache_key, response)
                
                soup = BeautifulSoup(response.content, 'html.parser')
                print("âœ… æˆåŠŸè·å–æœ€æ–°æ–‡ç« é¡µé¢")
                return soup
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è·å–æœ€æ–°æ–‡ç« é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    print("âŒ æ— æ³•è·å–æœ€æ–°æ–‡ç« é¡µé¢")
                    return None
    
    def extract_latest_article_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """æå–æœ€æ–°æ–‡ç« é“¾æ¥"""
        articles = []
        
        try:
            print("ğŸ” å¼€å§‹æå–æœ€æ–°æ–‡ç« é“¾æ¥...")
            
            # å¤šç§é€‰æ‹©å™¨ç­–ç•¥ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°æ–‡ç« 
            selectors = [
                # ä¸»è¦é€‰æ‹©å™¨
                ('#index_feeds_list a[href*="/articles/"]', "ä¸»è¦æ–‡ç« åˆ—è¡¨"),
                ('.article-list a[href*="/articles/"]', "æ–‡ç« åˆ—è¡¨"),
                ('.news-list a[href*="/articles/"]', "æ–°é—»åˆ—è¡¨"),
                ('.feed-list a[href*="/articles/"]', "Feedåˆ—è¡¨"),
                
                # å¤‡ç”¨é€‰æ‹©å™¨
                ('a[href*="/articles/"]', "æ‰€æœ‰æ–‡ç« é“¾æ¥"),
                ('article a[href*="article"]', "æ–‡ç« æ ‡ç­¾å†…é“¾æ¥"),
                ('.post a[href*="post"]', "å¸–å­é“¾æ¥"),
                ('.content a[href*="article"]', "å†…å®¹åŒºåŸŸé“¾æ¥"),
                
                # æ›´é€šç”¨çš„é€‰æ‹©å™¨
                ('a[href*="article"]', "åŒ…å«articleçš„é“¾æ¥"),
                ('a[href*="post"]', "åŒ…å«postçš„é“¾æ¥"),
                ('.article-item a', "æ–‡ç« é¡¹é“¾æ¥"),
                ('.news-item a', "æ–°é—»é¡¹é“¾æ¥")
            ]
            
            article_elements = []
            used_selector = None
            
            for selector, description in selectors:
                elements = soup.select(selector)
                if elements:
                    article_elements = elements[:10]  # å–å‰10ä¸ªä½œä¸ºå€™é€‰
                    used_selector = selector
                    print(f"ğŸ“° é€šè¿‡é€‰æ‹©å™¨æ‰¾åˆ° {len(article_elements)} ä¸ªå€™é€‰æ–‡ç« : {description} ({selector})")
                    break
            
            if not article_elements:
                print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥ï¼Œåˆ†æé¡µé¢ç»“æ„...")
                self._analyze_page_structure(soup)
                return []
            
            # å¤„ç†æ‰¾åˆ°çš„æ–‡ç« å…ƒç´ 
            for i, element in enumerate(article_elements):
                try:
                    article_info = self._parse_article_element(element, i)
                    if article_info:
                        articles.append(article_info)
                        print(f"âœ… æå–æ–‡ç«  {i+1}: {article_info['title'][:40]}...")
                    
                except Exception as e:
                    print(f"âš ï¸ è§£ææ–‡ç« å…ƒç´  {i+1} å¤±è´¥: {e}")
                    continue
            
            # æŒ‰æ—¶é—´æ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰å¹¶å–å‰3ç¯‡
            articles = self._sort_articles_by_date(articles)[:3]
            
            print(f"ğŸ“‹ æˆåŠŸæå– {len(articles)} ç¯‡æœ€æ–°æ–‡ç« ")
            return articles
            
        except Exception as e:
            print(f"âŒ æå–æ–‡ç« é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def _parse_article_element(self, element, index: int) -> Optional[Dict[str, str]]:
        """è§£ææ–‡ç« å…ƒç´ """
        try:
            # ç¡®ä¿æ˜¯é“¾æ¥å…ƒç´ 
            link_elem = element if element.name == 'a' else element.find('a')
            if not link_elem:
                return None
            
            href = link_elem.get('href')
            if not href:
                return None
            
            # æ„å»ºå®Œæ•´URL
            full_url = urljoin(LOOKONCHAIN_BASE_URL, href)
            
            # æå–æ ‡é¢˜
            title = self._extract_title_from_element(link_elem, element)
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_time = self._extract_publish_time(element)
            
            # éªŒè¯æ–‡ç« URLæ ¼å¼
            if '/articles/' not in href and 'article' not in href.lower():
                return None
            
            if not title or len(title.strip()) < 10:
                return None
            
            # ç”Ÿæˆæ–‡ç« ID
            article_id = hashlib.md5(full_url.encode()).hexdigest()[:12]
            
            article_info = {
                'title': title.strip(),
                'url': full_url,
                'publish_time': publish_time,
                'index': index,
                'id': article_id
            }
            
            return article_info
            
        except Exception as e:
            print(f"âš ï¸ è§£ææ–‡ç« å…ƒç´ å¤±è´¥: {e}")
            return None
    
    def _extract_title_from_element(self, link_elem, parent_element) -> str:
        """ä»å…ƒç´ ä¸­æå–æ ‡é¢˜"""
        # é¦–å…ˆå°è¯•é“¾æ¥æ–‡æœ¬
        title = link_elem.get_text().strip()
        
        # å¦‚æœæ ‡é¢˜å¤ªçŸ­ï¼Œå°è¯•ä»çˆ¶å…ƒç´ è·å–
        if len(title) < 10:
            # å°è¯•ä¸åŒçš„æ ‡é¢˜é€‰æ‹©å™¨
            title_selectors = [
                '.title', '[class*="title"]', 
                'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                '.headline', '[class*="headline"]',
                '.post-title', '.article-title'
            ]
            
            for selector in title_selectors:
                title_elem = parent_element.select_one(selector)
                if title_elem:
                    candidate_title = title_elem.get_text().strip()
                    if len(candidate_title) > 10:
                        title = candidate_title
                        break
            
            # å¦‚æœè¿˜æ˜¯å¤ªçŸ­ï¼Œå°è¯•è·å–çˆ¶å…ƒç´ çš„æ–‡æœ¬å†…å®¹
            if len(title) < 10:
                parent_text = parent_element.get_text().strip()
                # æå–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
                lines = [line.strip() for line in parent_text.split('\n') if line.strip()]
                if lines and len(lines[0]) > 10:
                    title = lines[0]
        
        # æ¸…ç†æ ‡é¢˜
        title = self._clean_title(title)
        
        return title
    
    def _clean_title(self, title: str) -> str:
        """æ¸…ç†æ ‡é¢˜"""
        import re
        
        # ç§»é™¤æ—¥æœŸæ ¼å¼
        title = re.sub(r'\d{4}\.\d{2}\.\d{2}', '', title)
        title = re.sub(r'\d{4}-\d{2}-\d{2}', '', title)
        
        # ç§»é™¤æ—¶é—´æ ¼å¼
        title = re.sub(r'\d{1,2}:\d{2}(?::\d{2})?', '', title)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        title = ' '.join(title.split())
        
        return title.strip()
    
    def _extract_publish_time(self, element) -> Optional[str]:
        """æå–å‘å¸ƒæ—¶é—´"""
        import re
        
        # å°è¯•æ‰¾åˆ°æ—¶é—´å…ƒç´ 
        time_selectors = [
            'time', '.time', '[class*="time"]', 
            '.date', '[class*="date"]',
            '.published', '[class*="published"]'
        ]
        
        for selector in time_selectors:
            time_elem = element.select_one(selector)
            if time_elem:
                time_text = time_elem.get_text().strip()
                
                # å°è¯•è§£ææ—¶é—´æ ¼å¼
                time_patterns = [
                    r'(\d{4}\.\d{2}\.\d{2})',
                    r'(\d{4}-\d{2}-\d{2})',
                    r'(\d{2}/\d{2}/\d{4})',
                    r'(\d{2}\.\d{2}\.\d{4})'
                ]
                
                for pattern in time_patterns:
                    match = re.search(pattern, time_text)
                    if match:
                        return match.group(1)
        
        return None
    
    def _sort_articles_by_date(self, articles: List[Dict]) -> List[Dict]:
        """æŒ‰æ—¥æœŸæ’åºæ–‡ç« """
        def get_sort_key(article):
            # ä¼˜å…ˆä½¿ç”¨å‘å¸ƒæ—¶é—´ï¼Œå…¶æ¬¡ä½¿ç”¨ç´¢å¼•
            publish_time = article.get('publish_time')
            if publish_time:
                # å°†å„ç§æ—¥æœŸæ ¼å¼ç»Ÿä¸€ä¸ºå¯æ¯”è¾ƒçš„æ ¼å¼
                try:
                    if '.' in publish_time:
                        date_obj = datetime.strptime(publish_time, '%Y.%m.%d')
                    elif '-' in publish_time:
                        date_obj = datetime.strptime(publish_time, '%Y-%m-%d')
                    elif '/' in publish_time:
                        date_obj = datetime.strptime(publish_time, '%m/%d/%Y')
                    else:
                        date_obj = datetime.now()
                    
                    return (date_obj, article.get('index', 999))
                    
                except ValueError:
                    pass
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨ç´¢å¼•
            return (datetime.min, article.get('index', 999))
        
        return sorted(articles, key=get_sort_key, reverse=True)
    
    def _analyze_page_structure(self, soup: BeautifulSoup):
        """åˆ†æé¡µé¢ç»“æ„ï¼Œç”¨äºè°ƒè¯•"""
        print("ğŸ” é¡µé¢ç»“æ„åˆ†æ:")
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"article"çš„é“¾æ¥
        article_links = soup.select('a[href*="article"]')
        print(f"   åŒ…å«'article'çš„é“¾æ¥: {len(article_links)} ä¸ª")
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"/articles/"çš„é“¾æ¥
        articles_links = soup.select('a[href*="/articles/"]')
        print(f"   åŒ…å«'/articles/'çš„é“¾æ¥: {len(articles_links)} ä¸ª")
        
        # æ˜¾ç¤ºé¡µé¢å‰10ä¸ªé“¾æ¥
        all_links = soup.select('a[href]')[:10]
        print("   é¡µé¢å‰10ä¸ªé“¾æ¥:")
        for i, link in enumerate(all_links, 1):
            href = link.get('href', '')[:80]
            text = link.get_text().strip()[:50]
            print(f"     {i}. {href} - {text}")
    
    def get_article_content_enhanced(self, article_url: str) -> Optional[str]:
        """å¢å¼ºçš„æ–‡ç« å†…å®¹è·å–"""
        cache_key = f"article_content_{hashlib.md5(article_url.encode()).hexdigest()[:8]}"
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_response = self._get_cached_response(cache_key)
        if cached_response:
            return self._extract_content_from_response(cached_response)
        
        # æ²¡æœ‰ç¼“å­˜ï¼Œå‘èµ·è¯·æ±‚
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸ“– æ­£åœ¨è·å–æ–‡ç« å†…å®¹: {article_url}")
                response = self.session.get(article_url, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()
                
                # ç¼“å­˜å“åº”
                self._cache_response(cache_key, response)
                
                content = self._extract_content_from_response(response)
                
                if content:
                    print(f"âœ… æˆåŠŸè·å–æ–‡ç« å†…å®¹: {len(content)} å­—ç¬¦")
                    return content
                else:
                    print(f"âš ï¸ æ— æ³•ä»é¡µé¢æå–æœ‰æ•ˆå†…å®¹")
                    return None
                
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è·å–æ–‡ç« å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    print(f"âŒ æ— æ³•è·å–æ–‡ç« : {article_url}")
                    return None
    
    def _extract_content_from_response(self, response: requests.Response) -> Optional[str]:
        """ä»å“åº”ä¸­æå–å†…å®¹"""
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 
                           '.ads', '.advertisement', '.sidebar', '.menu', '.navigation',
                           '.social-share', '.share-buttons', '.comments', '.related-posts']):
            element.decompose()
        
        # å°è¯•å¤šç§å†…å®¹æå–ç­–ç•¥
        content = self._extract_content_with_strategies(soup)
        
        if not content or len(content) < ARTICLE_MIN_LENGTH:
            return None
        
        if len(content) > ARTICLE_MAX_LENGTH:
            content = content[:ARTICLE_MAX_LENGTH] + "..."
        
        return content
    
    def _extract_content_with_strategies(self, soup: BeautifulSoup) -> str:
        """ä½¿ç”¨å¤šç§ç­–ç•¥æå–å†…å®¹"""
        # ç­–ç•¥1ï¼šä¸“ç”¨é€‰æ‹©å™¨
        strategies = [
            # LookOnChainä¸“ç”¨é€‰æ‹©å™¨
            ('.article-content', "æ–‡ç« å†…å®¹"),
            ('.post-content', "å¸–å­å†…å®¹"),
            ('.entry-content', "æ¡ç›®å†…å®¹"),
            ('.content-body', "å†…å®¹ä¸»ä½“"),
            ('#article-content', "æ–‡ç« å†…å®¹ID"),
            ('#post-content', "å¸–å­å†…å®¹ID"),
            
            # é€šç”¨é€‰æ‹©å™¨
            ('article', "æ–‡ç« æ ‡ç­¾"),
            ('.content', "å†…å®¹ç±»"),
            ('.main-content', "ä¸»è¦å†…å®¹"),
            ('main', "ä¸»è¦æ ‡ç­¾"),
            ('[role="main"]', "ä¸»è¦è§’è‰²"),
            ('.post-body', "å¸–å­ä¸»ä½“"),
            ('.article-body', "æ–‡ç« ä¸»ä½“")
        ]
        
        for selector, description in strategies:
            elements = soup.select(selector)
            for elem in elements:
                content = elem.get_text(separator=' ', strip=True)
                if len(content) >= ARTICLE_MIN_LENGTH:
                    print(f"ğŸ¯ ä½¿ç”¨ç­–ç•¥ '{description}': {selector}")
                    return content
        
        # ç­–ç•¥2ï¼šæ™ºèƒ½è¿‡æ»¤
        return self._extract_with_intelligent_filtering(soup)
    
    def _extract_with_intelligent_filtering(self, soup: BeautifulSoup) -> str:
        """æ™ºèƒ½è¿‡æ»¤æå–å†…å®¹"""
        body = soup.find('body')
        if not body:
            return ""
        
        raw_content = body.get_text(separator=' ', strip=True)
        
        # ç§»é™¤å™ªå£°
        import re
        
        noise_patterns = [
            r'Lookonchain\s*/\s*\d{4}\.\d{2}\.\d{2}',
            r'X\s+å…³æ³¨Telegram\s+åŠ å…¥',
            r'\d{4}\.\d{2}\.\d{2}\s+\d{2}:\d{2}:\d{2}',
            r'Follow\s+us\s+on\s+(Twitter|Telegram|X)',
            r'Join\s+our\s+(community|channel|group)',
            r'(Home|Login|Register|About|Contact|Menu|Navigation|Footer|Header|Search|Subscribe|Follow|Share|Like|Reply|Retweet|Tweet|Copy link|Download|Upload|Settings|Profile|Dashboard|Notifications|APP|åº”ç”¨å•†åº—|ç™»å½•|æ³¨å†Œ|é…ç½®æ–‡ä»¶|å®‰å…¨|æ³¨é”€|åŠ¨æ€|æ–‡ç« |æœç´¢å†å²|æ¸…é™¤å…¨éƒ¨|è¶‹åŠ¿æœç´¢|å…³æ³¨æˆ‘ä»¬|åŠ å…¥|ä¸‹è½½å›¾ç‰‡|å¤åˆ¶é“¾æ¥|ç›¸å…³å†…å®¹|åŸæ–‡|çƒ­ç‚¹æ–°é—»|æ›´å¤šçƒ­é—¨æ–‡ç« |æ›´å¤š)',
            r'(trending|popular|latest|hot|new|more|read more|continue reading|click here|learn more|show more|load more|view all|see all)'
        ]
        
        filtered_content = raw_content
        for pattern in noise_patterns:
            filtered_content = re.sub(pattern, ' ', filtered_content, flags=re.IGNORECASE)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        filtered_content = ' '.join(filtered_content.split())
        
        return filtered_content
    
    def scrape_latest_articles(self) -> List[Dict[str, str]]:
        """çˆ¬å–æœ€æ–°3ç¯‡æ–‡ç« """
        print("ğŸš€ å¼€å§‹çˆ¬å–æœ€æ–°LookOnChainæ–‡ç« ...")
        
        # è·å–æœ€æ–°æ–‡ç« é¡µé¢
        soup = self.get_latest_articles_page()
        if not soup:
            return []
        
        # æå–æ–‡ç« é“¾æ¥
        article_links = self.extract_latest_article_links(soup)
        if not article_links:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
            return []
        
        print(f"ğŸ“š å‡†å¤‡è·å– {len(article_links)} ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹")
        
        # è·å–æ¯ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹
        complete_articles = []
        for i, article_info in enumerate(article_links, 1):
            print(f"\nğŸ“° è·å–æ–‡ç«  {i}: {article_info['title'][:50]}...")
            
            content = self.get_article_content_enhanced(article_info['url'])
            if content:
                article_info['content'] = content
                complete_articles.append(article_info)
                print(f"âœ… æ–‡ç«  {i} è·å–å®Œæˆ")
            else:
                print(f"âŒ æ–‡ç«  {i} å†…å®¹è·å–å¤±è´¥")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if i < len(article_links):
                time.sleep(1)
        
        print(f"\nğŸ‰ æˆåŠŸçˆ¬å– {len(complete_articles)} ç¯‡æœ€æ–°æ–‡ç« ")
        return complete_articles
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()