"""
Hugo æ–‡ç« ç”Ÿæˆå™¨æ¨¡å—
å°†ç¿»è¯‘åçš„æ–‡ç« ç”Ÿæˆç¬¦åˆ Hugo æ ¼å¼çš„ markdown æ–‡ä»¶
"""

import os
import datetime
import json
import re
import hashlib
from typing import Dict, List, Optional, Set
from .config import (
    AUTHOR_INFO, DEFAULT_TAGS, DEFAULT_CATEGORIES, DEFAULT_KEYWORDS,
    DATA_DIR, LOOKONCHAIN_HISTORY_FILE
)
from .professional_formatter import ProfessionalFormatter


class ArticleGenerator:
    """Hugo æ–‡ç« ç”Ÿæˆå™¨"""
    
    def __init__(self, openai_api_key: str = None, logger=None):
        self.ensure_data_directory()
        self.history_file = LOOKONCHAIN_HISTORY_FILE
        self.content_history_file = os.path.join(DATA_DIR, 'content_hashes.json')
        # åˆå§‹åŒ–ä¸“ä¸šæ ¼å¼åŒ–å™¨
        self.formatter = ProfessionalFormatter(openai_api_key, logger)
        print("âœ… ArticleGeneratoråˆå§‹åŒ–å®Œæˆ")
    
    def ensure_data_directory(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        os.makedirs(DATA_DIR, exist_ok=True)
    
    def load_article_history(self) -> Set[str]:
        """åŠ è½½å·²ç”Ÿæˆçš„æ–‡ç« å†å²è®°å½•"""
        try:
            if os.path.exists(self.history_file):
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('generated_articles', []))
            return set()
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ–‡ç« å†å²è®°å½•å¤±è´¥: {e}")
            return set()
    
    def load_content_history(self) -> Dict[str, Dict]:
        """åŠ è½½å†…å®¹å“ˆå¸Œå†å²è®°å½•ï¼Œç”¨äºæ£€æµ‹é‡å¤å†…å®¹"""
        try:
            if os.path.exists(self.content_history_file):
                with open(self.content_history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get('content_hashes', {})
            return {}
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å†…å®¹å†å²è®°å½•å¤±è´¥: {e}")
            return {}
    
    def save_content_history(self, content_history: Dict[str, Dict]):
        """ä¿å­˜å†…å®¹å“ˆå¸Œå†å²è®°å½•"""
        try:
            data = {
                'last_updated': datetime.datetime.now().isoformat(),
                'total_hashes': len(content_history),
                'content_hashes': content_history
            }
            with open(self.content_history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²ä¿å­˜ {len(content_history)} ä¸ªå†…å®¹å“ˆå¸Œè®°å½•")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å†…å®¹å†å²è®°å½•å¤±è´¥: {e}")
    
    def generate_content_hash(self, content: str, title: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œï¼Œç”¨äºæ£€æµ‹é‡å¤å†…å®¹"""
        # ç»„åˆæ ‡é¢˜å’Œå†…å®¹ç”Ÿæˆå“ˆå¸Œ
        combined_text = f"{title}{content}"
        return hashlib.md5(combined_text.encode('utf-8')).hexdigest()
    
    def generate_semantic_hash(self, content: str) -> str:
        """ç”Ÿæˆè¯­ä¹‰å“ˆå¸Œï¼Œç”¨äºæ£€æµ‹ç›¸ä¼¼å†…å®¹"""
        # æå–å…³é”®è¯å’Œå…³é”®ä¿¡æ¯
        import re
        
        # æå–æ•°å­—ï¼ˆé‡‘é¢ã€æ•°é‡ç­‰ï¼‰
        numbers = re.findall(r'\$[\d,]+|\d+\s*(USD|ç¾å…ƒ|ç™¾ä¸‡|äº¿|ä¸‡|BTC|ETH)', content)
        numbers = sorted(set(numbers))  # å»é‡æ’åº
        
        # æå–åŠ å¯†è´§å¸å…³é”®è¯
        crypto_terms = ['æ¯”ç‰¹å¸', 'BTC', 'ä»¥å¤ªåŠ', 'ETH', 'DeFi', 'NFT', 'ä»£å¸', 'äº¤æ˜“æ‰€', 'é’±åŒ…', 'åœ°å€']
        found_terms = [term for term in crypto_terms if term in content]
        
        # æå–æ—¶é—´ä¿¡æ¯
        dates = re.findall(r'\d{4}\.\d{2}\.\d{2}|\d{1,2}\s*æœˆ|\d{1,2}\s*æ—¥', content)
        dates = sorted(set(dates))
        
        # ç»„åˆå…³é”®ä¿¡æ¯ç”Ÿæˆè¯­ä¹‰å“ˆå¸Œ
        semantic_content = f"{'|'.join(found_terms)}|{'|'.join(numbers)}|{'|'.join(dates)}"
        return hashlib.md5(semantic_content.encode('utf-8')).hexdigest()
    
    def is_duplicate_content(self, article: Dict[str, str], url_history: Set[str], content_history: Dict[str, Dict]) -> Dict[str, bool]:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å†…å®¹"""
        article_id = article['id']
        title = article.get('chinese_title', article.get('original_title', ''))
        content = article.get('chinese_content', article.get('original_content', ''))
        
        duplicate_info = {
            'url_duplicate': False,
            'content_duplicate': False,
            'semantic_duplicate': False,
            'is_duplicate': False
        }
        
        # 1. æ£€æŸ¥URLé‡å¤
        if article_id in url_history:
            duplicate_info['url_duplicate'] = True
            print(f"âš ï¸ æ£€æµ‹åˆ°URLé‡å¤: {title[:30]}...")
        
        # 2. æ£€æŸ¥å†…å®¹å“ˆå¸Œé‡å¤
        content_hash = self.generate_content_hash(content, title)
        if content_hash in content_history:
            duplicate_info['content_duplicate'] = True
            existing_record = content_history[content_hash]
            print(f"âš ï¸ æ£€æµ‹åˆ°å†…å®¹é‡å¤: {title[:30]}... (ä¸ {existing_record.get('title', 'unknown')} é‡å¤)")
        
        # 3. æ£€æŸ¥è¯­ä¹‰é‡å¤
        semantic_hash = self.generate_semantic_hash(content)
        if semantic_hash in content_history:
            existing_record = content_history[semantic_hash]
            # æ£€æŸ¥æ—¶é—´é—´éš”ï¼Œé¿å…è¯¯åˆ¤æ—¶æ•ˆæ€§å†…å®¹
            existing_date = existing_record.get('date', '')
            if existing_date:
                try:
                    from datetime import datetime as dt
                    existing_dt = dt.fromisoformat(existing_date.replace('Z', '+00:00'))
                    current_dt = datetime.datetime.now()
                    days_diff = (current_dt - existing_dt).days
                    
                    # å¦‚æœè¶…è¿‡7å¤©ï¼Œè®¤ä¸ºæ˜¯ä¸åŒçš„å†…å®¹
                    if days_diff <= 7:
                        duplicate_info['semantic_duplicate'] = True
                        print(f"âš ï¸ æ£€æµ‹åˆ°è¯­ä¹‰é‡å¤: {title[:30]}... (ä¸ {existing_record.get('title', 'unknown')} ç›¸ä¼¼ï¼Œé—´éš” {days_diff} å¤©)")
                except:
                    # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œä¿å®ˆåˆ¤æ–­ä¸ºé‡å¤
                    duplicate_info['semantic_duplicate'] = True
                    print(f"âš ï¸ æ£€æµ‹åˆ°è¯­ä¹‰é‡å¤: {title[:30]}... (ä¸ {existing_record.get('title', 'unknown')} ç›¸ä¼¼)")
        
        # ç»¼åˆåˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤
        duplicate_info['is_duplicate'] = (
            duplicate_info['url_duplicate'] or 
            duplicate_info['content_duplicate'] or 
            duplicate_info['semantic_duplicate']
        )
        
        return duplicate_info
    
    def add_to_content_history(self, article: Dict[str, str], content_history: Dict[str, Dict]):
        """æ·»åŠ åˆ°å†…å®¹å†å²è®°å½•"""
        title = article.get('chinese_title', article.get('original_title', ''))
        content = article.get('chinese_content', article.get('original_content', ''))
        article_id = article['id']
        
        # ç”Ÿæˆå“ˆå¸Œ
        content_hash = self.generate_content_hash(content, title)
        semantic_hash = self.generate_semantic_hash(content)
        
        # è®°å½•ä¿¡æ¯
        record = {
            'id': article_id,
            'title': title,
            'url': article.get('url', ''),
            'date': datetime.datetime.now().isoformat(),
            'content_hash': content_hash,
            'semantic_hash': semantic_hash
        }
        
        # ä¿å­˜ä¸¤ç§å“ˆå¸Œ
        content_history[content_hash] = record
        content_history[semantic_hash] = record
        
        print(f"ğŸ“ å·²æ·»åŠ åˆ°å†…å®¹å†å²: {title[:30]}...")
    
    def save_article_history(self, article_ids: Set[str]):
        """ä¿å­˜å·²ç”Ÿæˆçš„æ–‡ç« å†å²è®°å½•"""
        try:
            data = {
                'last_updated': datetime.datetime.now().isoformat(),
                'total_articles': len(article_ids),
                'generated_articles': list(article_ids)
            }
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²ä¿å­˜ {len(article_ids)} ä¸ªæ–‡ç« è®°å½•")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ–‡ç« å†å²è®°å½•å¤±è´¥: {e}")
    
    def is_article_generated(self, article_id: str, generated_articles: Set[str]) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²ç»ç”Ÿæˆè¿‡"""
        return article_id in generated_articles
    
    def generate_filename(self, chinese_title: str, article_id: str) -> str:
        """ç”Ÿæˆæ–‡ä»¶å"""
        # ä»ä¸­æ–‡æ ‡é¢˜ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        filename_base = re.sub(r'[^\w\u4e00-\u9fa5\-]', '-', chinese_title)
        filename_base = re.sub(r'-+', '-', filename_base).strip('-')
        
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(filename_base) > 50:
            filename_base = filename_base[:50]
        
        # æ·»åŠ æ—¥æœŸå’ŒIDç¡®ä¿å”¯ä¸€æ€§
        today = datetime.datetime.now().strftime('%Y-%m-%d')
        filename = f"lookonchain-{filename_base}-{today}-{article_id[:8]}.md"
        
        return filename
    
    def generate_english_slug(self, chinese_title: str, date_obj: datetime.datetime) -> str:
        """åŸºäºä¸­æ–‡æ ‡é¢˜ç”Ÿæˆè‹±æ–‡slug"""
        date_str = date_obj.strftime('%Y-%m-%d')
        
        # å¸¸è§åŠ å¯†è´§å¸æœ¯è¯­æ˜ å°„
        crypto_terms = {
            'æ¯”ç‰¹å¸': 'bitcoin',
            'BTC': 'btc', 
            'ä»¥å¤ªåŠ': 'ethereum',
            'ETH': 'eth',
            'é²¸é±¼': 'whale',
            'åœ°å€': 'address',
            'äº¤æ˜“': 'transaction',
            'èµ„é‡‘': 'fund',
            'è½¬è´¦': 'transfer',
            'æµå…¥': 'inflow',
            'æµå‡º': 'outflow',
            'äº¤æ˜“æ‰€': 'exchange',
            'å¸å®‰': 'binance',
            'æŠ›å”®': 'sell',
            'ä¹°å…¥': 'buy',
            'æŒä»“': 'position',
            'DeFi': 'defi',
            'NFT': 'nft',
            'ä»£å¸': 'token',
            'é¡¹ç›®': 'project',
            'åˆ†æ': 'analysis',
            'å¸‚åœº': 'market',
            'ä»·æ ¼': 'price',
            'æŠ•èµ„': 'investment',
            'é“¾ä¸Š': 'onchain',
            'æ•°æ®': 'data',
            'ç›‘æ§': 'monitoring',
            'USDT': 'usdt',
            'USDC': 'usdc'
        }
        
        # æå–å…³é”®è¯å¹¶è½¬æ¢ä¸ºè‹±æ–‡
        title_lower = chinese_title.lower()
        english_parts = []
        
        # åŒ¹é…å…³é”®æœ¯è¯­
        for chinese, english in crypto_terms.items():
            if chinese.lower() in title_lower:
                english_parts.append(english)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šæœ¯è¯­ï¼Œä½¿ç”¨é€šç”¨æè¿°
        if not english_parts:
            if 'åˆ†æ' in title_lower or 'æ•°æ®' in title_lower:
                english_parts.append('analysis')
            if 'äº¤æ˜“' in title_lower or 'è½¬è´¦' in title_lower:
                english_parts.append('transaction')
            if 'åœ°å€' in title_lower:
                english_parts.append('address')
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not english_parts:
            english_parts.append('crypto-data')
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        english_parts = list(dict.fromkeys(english_parts))[:3]
        
        # ç”Ÿæˆslug
        slug_base = '-'.join(english_parts)
        slug = f"lookonchain-{slug_base}-{date_str}"
        
        # æ¸…ç†slugï¼ˆç¡®ä¿åªåŒ…å«å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ï¼‰
        slug = re.sub(r'[^a-z0-9\-]', '', slug.lower())
        slug = re.sub(r'-+', '-', slug).strip('-')
        
        return slug

    def generate_article_tags(self, chinese_content: str, chinese_title: str) -> List[str]:
        """åŸºäºå†…å®¹ç”Ÿæˆç›¸å…³æ ‡ç­¾"""
        tags = DEFAULT_TAGS.copy()
        
        # å…³é”®è¯æ£€æµ‹
        content_lower = (chinese_content + chinese_title).lower()
        
        keyword_mapping = {
            'defi': ['DeFi', 'å»ä¸­å¿ƒåŒ–é‡‘è'],
            'nft': ['NFT', 'æ•°å­—æ”¶è—å“'],
            'bitcoin': ['Bitcoin', 'BTC', 'æ¯”ç‰¹å¸'],
            'ethereum': ['Ethereum', 'ETH', 'ä»¥å¤ªåŠ'],
            'whale': ['é²¸é±¼åœ°å€', 'å¤§æˆ·'],
            'exchange': ['äº¤æ˜“æ‰€', 'ä¸­å¿ƒåŒ–äº¤æ˜“'],
            'dao': ['DAO', 'å»ä¸­å¿ƒåŒ–ç»„ç»‡'],
            'token': ['ä»£å¸åˆ†æ', 'ä»£å¸ç»æµå­¦'],
            'mining': ['æŒ–çŸ¿', 'çŸ¿å·¥'],
            'staking': ['è´¨æŠ¼', 'æƒç›Šè¯æ˜']
        }
        
        for keyword, related_tags in keyword_mapping.items():
            if keyword in content_lower:
                tags.extend(related_tags)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        return list(set(tags))[:8]
    
    def generate_hugo_frontmatter(self, article: Dict[str, str]) -> str:
        """ç”ŸæˆHugoå‰ç½®matter"""
        
        now = datetime.datetime.now()
        date_str = now.strftime('%Y-%m-%dT%H:%M:%S+08:00')
        
        # å¤„ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_title = article['chinese_title'].replace("'", "''").replace('"', '""')
        
        # ç”Ÿæˆè‹±æ–‡slug
        slug = self.generate_english_slug(article['chinese_title'], now)
        
        # ç”Ÿæˆæè¿°
        description = article['summary'][:150] + "..." if len(article['summary']) > 150 else article['summary']
        safe_description = description.replace("'", "''").replace('"', '""')
        
        # ç”Ÿæˆæ ‡ç­¾
        tags = self.generate_article_tags(article['chinese_content'], article['chinese_title'])
        tags_str = str(tags).replace("'", '"')  # ä½¿ç”¨åŒå¼•å·
        
        # ç”Ÿæˆå…³é”®è¯
        keywords = DEFAULT_KEYWORDS.copy()
        keywords.append(f"{article['chinese_title']}åˆ†æ")
        keywords_str = str(keywords).replace("'", '"')
        
        # ç”Ÿæˆåˆ†ç±»
        categories_str = str(DEFAULT_CATEGORIES).replace("'", '"')
        
        frontmatter = f"""+++
date = '{date_str}'
draft = false
title = '{safe_title}'
description = '{safe_description}'
summary = '{safe_description}'
slug = '{slug}'
tags = {tags_str}
categories = {categories_str}
keywords = {keywords_str}
author = '{AUTHOR_INFO["name"]}'
ShowToc = true
TocOpen = false
ShowReadingTime = true
ShowBreadCrumbs = true
ShowPostNavLinks = true
ShowWordCount = true
ShowShareButtons = true

[cover]
image = ""
alt = "LookOnChainé“¾ä¸Šæ•°æ®åˆ†æ"
caption = "æ·±åº¦è§£æåŒºå—é“¾é“¾ä¸Šæ•°æ®åŠ¨æ€"
relative = false
hidden = false
+++"""
        
        return frontmatter
    
    def generate_article_content(self, article: Dict[str, str]) -> str:
        """ç”Ÿæˆå®Œæ•´çš„æ–‡ç« å†…å®¹"""
        
        # é¦–å…ˆå°è¯•ä¸“ä¸šæ ¼å¼åŒ–
        if self.formatter and self.formatter.client:
            print("ğŸ¨ ä½¿ç”¨ä¸“ä¸šæ ¼å¼åŒ–å™¨ç”Ÿæˆå†…å®¹...")
            formatted_article = self.formatter.format_content(article)
            if formatted_article and formatted_article.get('formatted_content'):
                return self._add_author_section(formatted_article['formatted_content'])
        
        print("ğŸ“ ä½¿ç”¨é»˜è®¤æ ¼å¼ç”Ÿæˆå†…å®¹...")
        # Fallback: ä½¿ç”¨åŸæœ‰çš„ç®€å•æ ¼å¼
        content = f"""{{{{< alert >}}}}
**LookOnChainé“¾ä¸Šç›‘æ§**: {article['summary']}
{{{{< /alert >}}}}

{article['chinese_content']}

## ğŸ“Š æ•°æ®æ¥æºä¸åˆ†æ

æœ¬æ–‡åŸºäºLookOnChainå¹³å°çš„é“¾ä¸Šæ•°æ®åˆ†æç”Ÿæˆã€‚LookOnChainæ˜¯ä¸šç•Œé¢†å…ˆçš„åŒºå—é“¾æ•°æ®åˆ†æå¹³å°ï¼Œä¸“æ³¨äºè¿½è¸ªå’Œåˆ†æé“¾ä¸Šèµ„é‡‘æµåŠ¨ã€å¤§æˆ·è¡Œä¸ºç­‰å…³é”®ä¿¡æ¯ã€‚

### ğŸ”— ç›¸å…³é“¾æ¥
- **åŸæ–‡é“¾æ¥**: [{article.get('url', '#')}]({article.get('url', '#')})
- **LookOnChainå¹³å°**: [https://www.lookonchain.com/](https://www.lookonchain.com/)

### ğŸ“ˆ æŠ•èµ„é£é™©æç¤º
ä»¥ä¸Šå†…å®¹ä»…ä¸ºé“¾ä¸Šæ•°æ®åˆ†æï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚åŠ å¯†è´§å¸æŠ•èµ„å­˜åœ¨é«˜é£é™©ï¼Œä»·æ ¼æ³¢åŠ¨å‰§çƒˆï¼Œè¯·ç†æ€§æŠ•èµ„å¹¶åšå¥½é£é™©ç®¡ç†ã€‚æŠ•èµ„å‰è¯·å……åˆ†äº†è§£é¡¹ç›®åŸºæœ¬é¢ï¼Œä¸è¦æŠ•å…¥è¶…å‡ºæ‰¿å—èƒ½åŠ›çš„èµ„é‡‘ã€‚"""
        
        return self._add_author_section(content)
    
    def _add_author_section(self, content: str) -> str:
        """æ·»åŠ ä½œè€…ä¿¡æ¯éƒ¨åˆ†"""
        author_section = f"""

---

## ğŸ“ å…³äºä½œè€…

**{AUTHOR_INFO['name']}** - {AUTHOR_INFO['title']}

### ğŸ”— è”ç³»æ–¹å¼ä¸å¹³å°

- **ğŸ“§ é‚®ç®±**: [{AUTHOR_INFO['email']}](mailto:{AUTHOR_INFO['email']})
- **ğŸ¦ Twitter**: [{AUTHOR_INFO['twitter']}](https://twitter.com/{AUTHOR_INFO['twitter'].replace('@', '')})
- **ğŸ’¬ å¾®ä¿¡**: {AUTHOR_INFO['wechat']}
- **ğŸ“± Telegram**: [{AUTHOR_INFO['telegram']}]({AUTHOR_INFO['telegram']})
- **ğŸ“¢ Telegramé¢‘é“**: [{AUTHOR_INFO['telegram_channel']}]({AUTHOR_INFO['telegram_channel']})
- **ğŸ‘¥ åŠ å¯†æƒ…æŠ¥TGç¾¤**: [{AUTHOR_INFO['telegram_group']}]({AUTHOR_INFO['telegram_group']})
- **ğŸ¥ YouTubeé¢‘é“**: [{AUTHOR_INFO['youtube']}]({AUTHOR_INFO['youtube']})

### ğŸŒ ç›¸å…³å¹³å°

- **ğŸ“Š åŠ å¯†è´§å¸ä¿¡æ¯èšåˆç½‘ç«™**: [{AUTHOR_INFO['website']}]({AUTHOR_INFO['website']})
- **ğŸ“– å…¬ä¼—å·**: {AUTHOR_INFO['wechat_public']}

*æ¬¢è¿å…³æ³¨æˆ‘çš„å„ä¸ªå¹³å°ï¼Œè·å–æœ€æ–°çš„åŠ å¯†è´§å¸å¸‚åœºåˆ†æå’ŒæŠ•èµ„æ´å¯Ÿï¼*"""
        
        return content + author_section
    
    def create_hugo_article(self, article: Dict[str, str], output_dir: str) -> Optional[str]:
        """åˆ›å»ºå®Œæ•´çš„Hugoæ–‡ç« æ–‡ä»¶"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            filename = self.generate_filename(article['chinese_title'], article['id'])
            file_path = os.path.join(output_dir, filename)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆå‰ç½®matter
            frontmatter = self.generate_hugo_frontmatter(article)
            
            # ç”Ÿæˆæ–‡ç« å†…å®¹
            content = self.generate_article_content(article)
            
            # ç»„åˆå®Œæ•´æ–‡ç« 
            full_article = f"{frontmatter}\n\n{content}\n"
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(full_article)
            
            print(f"âœ… æ–‡ç« å·²ç”Ÿæˆ: {file_path}")
            return file_path
            
        except Exception as e:
            print(f"âŒ æ–‡ç« ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def generate_daily_articles(self, processed_articles: List[Dict[str, str]]) -> Dict[str, any]:
        """ç”Ÿæˆå½“æ—¥çš„æ‰€æœ‰æ–‡ç« """
        if not processed_articles:
            return {"success": False, "message": "æ²¡æœ‰æ–‡ç« éœ€è¦ç”Ÿæˆ", "generated": 0}
        
        print(f"ğŸ“ å‡†å¤‡ç”Ÿæˆ {len(processed_articles)} ç¯‡æ–‡ç« ...")
        
        # åŠ è½½å†å²è®°å½•
        generated_articles = self.load_article_history()
        content_history = self.load_content_history()
        
        print(f"ğŸ“š å·²ç”Ÿæˆæ–‡ç« æ•°é‡: {len(generated_articles)}")
        print(f"ğŸ“‹ å†…å®¹å“ˆå¸Œè®°å½•æ•°é‡: {len(content_history)}")
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        current_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        output_dir = os.path.join(current_dir, 'content', 'posts')
        
        generated_count = 0
        generated_files = []
        newly_generated = set()
        skipped_duplicates = 0
        
        for i, article in enumerate(processed_articles, 1):
            article_id = article['id']
            
            # æ£€æŸ¥æ˜¯å¦å·²ç”Ÿæˆ
            if self.is_article_generated(article_id, generated_articles):
                print(f"â­ï¸ è·³è¿‡å·²ç”Ÿæˆæ–‡ç«  {i}: {article['chinese_title'][:30]}...")
                continue
            
            # æ£€æŸ¥é‡å¤å†…å®¹
            duplicate_info = self.is_duplicate_content(article, generated_articles, content_history)
            
            if duplicate_info['is_duplicate']:
                skipped_duplicates += 1
                print(f"ğŸš« è·³è¿‡é‡å¤æ–‡ç«  {i}: {article['chinese_title'][:30]}...")
                continue
            
            print(f"\nğŸ“„ ç”Ÿæˆæ–‡ç«  {i}: {article['chinese_title'][:50]}...")
            
            # ç”Ÿæˆæ–‡ç« 
            file_path = self.create_hugo_article(article, output_dir)
            if file_path:
                generated_count += 1
                generated_files.append(file_path)
                newly_generated.add(article_id)
                
                # æ·»åŠ åˆ°å†…å®¹å†å²è®°å½•
                self.add_to_content_history(article, content_history)
                
                print(f"âœ… æ–‡ç«  {i} ç”ŸæˆæˆåŠŸ")
            else:
                print(f"âŒ æ–‡ç«  {i} ç”Ÿæˆå¤±è´¥")
        
        # æ›´æ–°å†å²è®°å½•
        if newly_generated:
            all_generated = generated_articles.union(newly_generated)
            self.save_article_history(all_generated)
            
        # ä¿å­˜å†…å®¹å†å²è®°å½•
        if content_history:
            self.save_content_history(content_history)
        
        result = {
            "success": generated_count > 0,
            "generated": generated_count,
            "total_processed": len(processed_articles),
            "files": generated_files,
            "newly_generated_ids": list(newly_generated),
            "skipped_duplicates": skipped_duplicates
        }
        
        if generated_count > 0:
            result["message"] = f"æˆåŠŸç”Ÿæˆ {generated_count} ç¯‡æ–‡ç« ï¼Œè·³è¿‡ {skipped_duplicates} ç¯‡é‡å¤æ–‡ç« "
        elif skipped_duplicates > 0:
            result["message"] = f"æ‰€æœ‰æ–‡ç« éƒ½æ˜¯é‡å¤å†…å®¹ï¼Œè·³è¿‡ {skipped_duplicates} ç¯‡æ–‡ç« "
        else:
            result["message"] = "æ²¡æœ‰æ–°æ–‡ç« ç”Ÿæˆï¼ˆå¯èƒ½éƒ½å·²å­˜åœ¨ï¼‰"
        
        return result